
---
title: "FeatureSelection"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FeatureSelection}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Clearing the Confusion: How (and Why) Should We Select Just a Few of Our (Possible Many) Features?

In many applications we may have a large number of features---dozens,
hundreds or even more.  In machine learning contexts, where we are
interested primarily in prediction, we typically don't want to use all
the features, for a couple of reasons:

* *Avoiding overfitting.*

    As we add more and more features to a model, bias is reduced but
    variance increases.  At some point, variance overwhelms bias, and our
    predictive ability declines.

* *Avoiding large computation time and/or memory usage.*

    Depending on the solution method used, computation time for a linear
    model increases with at least the square of the number of features.  If
    say we have 3 times as many features in one model than another,
    computation time for the larger multiple will be roughly 9 times that of
    the smaller one.  Modern ML models can be especially heavy in terms of
    computational burden.  In addition, large models may result in lack of
    convergence or instability of the output.

# Which Method to Use to Select Features?

Many, many methods for feature selection have been developed over the
years, and you will often hear someone claim that their favorite is *the*
one to use, The Best.  As you may guess, the fact that there are
conflicting claims as to "best" reflects the fact that no such Best 
method exists.  Indeed, much of the feature selection process is *ad hoc*.

We will cover a few feature selection methods here, to explain their
rationale and how to use them in qeML.  You may typically use two or
more of them in any given application.

# How Many Is Too Many?

We will use standard notation here, with n and p denoting the number of
points in our dataset (i.e. number of rows) and the number of features
(i.e. number of columns), respectively.  Important points to remember:

## General principles

* The larger n is, then the larger a value of p that can be used before
  variance increase overwhelms bias reduction.

* However, that bias-variance tradeoff transition point for p depends on the
  application and the ML method. There is no magic formula for 
  best p as a function of n.

* Nevertheless, a commonly-used rule of thumb is to limit p to at most
  n<sup>0.5</sup>.  Some theoretical analyses even suggest the more
  conservative rule p < log(n).  Again, these are oversimplifications,
  but you may find them useful as guides to intuition.

* The value of p depends on "expanded" versions of any categorical features.
  A single categorical variable (R factor) with 10 levels with become 10
  dummy/"one hot" variables.  The true value of p must ake this into 
  account, as follows.

## Impact of categorical variables on p

In many large-p datasets, the culprit is primarily categorical
variables.  Let's see why. 

Consider the dataset **nycdata** included in qeML, which is
derived from taxi trip data made publicly available by the New York City
government.  Many public analyses have been made on various versions of
the NYC taxi data, frequently with the goal of predicting trip time.
Let's take a look:

``` r
> data(nyctaxi)
> dim(nyctaxi)
[1] 10000     5
> head(nyctaxi)
        trip_distance PULocationID DOLocationID tripTime DayOfWeek
2969561          1.37          236           43      598         1
7301968          0.71          238          238      224         4
3556729          2.80          100          263      761         3
7309631          2.62          161          249      888         4
3893911          1.20          236          163      648         5
4108506          2.40          161          164      977         5
```

(The 7-digit numbers on the left are row names.  Time-of-day, month etc.
data is also available but not in this particular dataset.)

So n = 10000, p = 4.  At first glance, one might guess that we could use
all 5 features.  Let's try a linear model:

``` r
> z <- qeLin(nyctaxi,'tripTime')
holdout set has  1000 rows
Warning messages:
1: In eval(tmp, parent.frame()) :
  7 rows removed from test set, due to new factor levels
2: In predict.lm(object, newx) :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newx) :
  prediction from a rank-deficient fit may be misleading
```

To understand those warning messages, let's look at the estimated beta
coefficients:

``` r
> summary(z)

Call:
lm(formula = cbind(tripTime) ~ ., data = xy)

Residuals:
    Min      1Q  Median      3Q     Max
-3741.0  -187.7   -49.1   133.0  3161.1 

Coefficients: (4 not defined because of singularities)
                 Estimate Std. Error t value Pr(>|t|)
trip_distance     159.650      1.664  95.943  < 2e-16 ***
PULocationID4    1261.444    347.127   3.634 0.000281 ***
PULocationID7    1090.701    349.090   3.124 0.001787 ** 
PULocationID10    555.575    372.021   1.493 0.135370    
PULocationID11   2224.466    526.747   4.223 2.44e-05 ***
PULocationID12   1723.856    409.102   4.214 2.54e-05 ***
PULocationID13   1131.894    339.186   3.337 0.000850 ***
PULocationID17   1003.560    478.219   2.099 0.035887 *  
PULocationID21   1272.560    453.341   2.807 0.005011 ** 
PULocationID24   1248.627    343.352   3.637 0.000278 ***
...
...
PULocationID263  1230.751    337.961   3.642 0.000272 ***
PULocationID264  1192.486    341.763   3.489 0.000487 ***
PULocationID265  1615.514    365.564   4.419 1.00e-05 ***
DOLocationID3     761.949    340.481   2.238 0.025255 *  
DOLocationID4     731.542    106.167   6.890 5.95e-12 ***
DOLocationID7     870.916    107.784   8.080 7.34e-16 ***
DOLocationID10   1396.929    211.307   6.611 4.05e-11 ***
DOLocationID11    505.716    340.286   1.486 0.137275    
DOLocationID12    616.109    216.917   2.840 0.004518 ** 
DOLocationID13    736.207     99.368   7.409 1.39e-13 ***
DOLocationID14    389.380    172.166   2.262 0.023744 *  
DOLocationID15    569.084    341.053   1.669 0.095231 .  
DOLocationID16    706.829    341.146   2.072 0.038302 *  
DOLocationID17    926.875    139.426   6.648 3.16e-11 ***
DOLocationID18    430.492    252.648   1.704 0.088432 .  
DOLocationID20         NA         NA      NA       NA    
DOLocationID21    339.841    292.195   1.163 0.244837    
DOLocationID22    165.218    209.337   0.789 0.429990    
DOLocationID24    780.893    109.440   7.135 1.04e-12 ***
...
...
DOLocationID258  1300.459    342.076   3.802 0.000145 ***
DOLocationID259  -343.183    340.179  -1.009 0.313085    
DOLocationID260   972.741    150.491   6.464 1.08e-10 ***
DOLocationID261   814.888    105.780   7.704 1.47e-14 ***
DOLocationID262   752.903     96.517   7.801 6.87e-15 ***
DOLocationID263   700.357     94.741   7.392 1.58e-13 ***
DOLocationID264   840.385    107.942   7.786 7.74e-15 ***
DOLocationID265  -127.962    123.014  -1.040 0.298264    
DayOfWeek2         45.682     15.025   3.040 0.002369 ** 
DayOfWeek3        107.253     14.236   7.534 5.43e-14 ***
DayOfWeek4        108.264     14.019   7.723 1.26e-14 ***
DayOfWeek5        139.051     13.736  10.123  < 2e-16 ***
DayOfWeek6        104.749     14.435   7.257 4.31e-13 ***
DayOfWeek7         66.211     14.549   4.551 5.41e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 327.6 on 8657 degrees of freedom
Multiple R-squared:  0.7056,	Adjusted R-squared:  0.694 
F-statistic: 60.68 on 342 and 8657 DF,  p-value: < 2.2e-16
```

Note the following:

* There were 266 pickup/dropoff locations.  That means 265 dummy-variable
features each for pickup and dropoff.  Similarly, there are 6 more dummies
for day of the week.  R's **lm** function, which **qeLin** wraps,
automatically converts factors to dummies.

    So, p is not 4 after all; it's 4 + 530 + 6 = 540!
    
    We are assuming no interactions here.  But if some pickup/dropoff
    location combinations behave quite differently from others, we ought to
    consider interaction terms.  These will have their own dummy variables,
    hugely increasing p if we include them all.

* Now, in those warning messages, the term "rank-deficient" is referring
to multicollinearity in the data, i.e. strong relations among the
features, producing a possibly unstable result.  Note that the model was
so unstable that the coefficient for DOLocationID20 turned out to be
NA.

* A more subtle problem is in the warning, "7 rows removed from test set,
due to new factor levels."  By default, all **qeML** predictive
functions split the data into training and holdout sets.  Say some
pickup location had only two rows in our data, neither of which happened to
be in the holdout set.  Then the **lm** function would say, "Wait, I've
never heard of these factor levels before," so **qeLin** removes them.
If many rows are removed, our predictive ability suffers.

# Desiderata

What should we look for in a "good" feature selection method?  We take
the following as goals:

* **Desired Property A:** The method should have predictive ability as a central goal.

* **Desired Property B:** The method should be reasonably easy to explain.

* **Desired Property C:** The method should produce an ordered sequence of
candidate models. 

    All the methods discussed here will have this last property. 
    This is so important that we will give it its own subsection:

## Feature selection methods should produce an ordered sequence of candidate models

A nethod should produce some kind of ordered list, for instance, best
single predictor; best pair of predictors; best triplet etc.  We can
evaluate each of the p feature sets via cross-validation, then choose
the one that peforms best.  (A method may simply rank features according
to importance, but we still would assess the performance of the first
feature, the first two features together, the first three features
together and so on.)

In the *all possible subsets* (APS) approach, by contrast, one looks at all
the 2<sup>p</sup> possible feature sets, a prohibitively large number
computationally.  

Moreover, remember, we are working with randomness.  We choose holdout
sets randomly, and usually the dataset itself is considered to be a
random sample from some (real or conceptual) population.  Due to this
randomness, it's possible that some combination of features will appear
to perform well, just by accident.  The more feature sets we look at,
the greater the chance of this occurring.  This consideration really
makes APS infeasible, and even with a linear sequence of candidate
feature sets, this can be problematic with large p.

Note that some of the methods discussed below do their own internal
cross-validation, and ultimately give us the "best" feature set
according to that criterion.  Nevertheless, they still provide an
ordered feature set.

This last point is important because we may wish to use the selected
feature set sequence as input to a different ML method.  We may, for
instance, first use the LASSO to produce an ordered sequence of
predictors, and then use that sequence with, say, a neural networks
model.  We would apply **qeNN** to each set of features in the sequence,
then choose whichever feature set has the smallest value of
**qeNN()$testAcc**.  There is no theoretical reason to believe the best
feature set for one ML method is also best for another, but as noted,
feature selection is an *ad hoc* business.

# Feature Selection Methodology Overview

## Methods based on p-values

A classic approach for linear and generalized linear models is to first
fit a full model, then retain only those features that are
"statistically significant".  In the **nyctaxi** taxi data shown above,
we would retain, for instance **PULocationID11** but not **PULocationID10**.

This would violate our Desired Property A above.  Use of p-values in
general is problematic (see the "NoPVals" vignette), and this is no
exception.  Just because &beta;<sub>i</sub> is nonzero does not mean
that Feature i has strong predictive power; what if  &beta;<sub>i</sub>
is nonzero but tiny?

One could generate an ordered sequence of feature sets in the above
manner by first setting the threshold for a p-value very high, then
progressively lower, bringing in more features each time we lower the
threshold.  This is similar to  *stepwise regression*:  In the
*forward* version, one starts with no features, but adds them one at a
time.  At each step, one tests for the &beta; coefficient being 0.  When
no new feature is found to be "significant," the process stops, and we
use whatever features managed to make it in to the model so far.  The
*backward* version starts with a full model, and removes features one at
a time.  Though this one may seem to be an improvement, in that it does
take into account that a feature may be useful individually but not if
similar features are already in the equation, it still suffers from the
fundamental problems of p-values.

## The LASSO

Here we fit a linear model, subject to the constraint that 
the &;<sub>i</sub> are "small."  Specifically, we require that 

&Sum; |b<sub>i</sub>| < &lambda;

where b<sub>i</sub> is our sample estimate of &beta;<sub>i.</sub>.

Here &lambda; is a hyperparameter.  Starting at 0, we increase &lambda;
to ever-larger values, having the effect that one estimated
&beta;<sub>i</sub> becomes nonzero at a time.  This gives us an ordered
sequence of candidate features, as desired.

This is similar to a forward stepwise method, but NOT based on p-values;
instead, the decision of which feature to use next is based on
cross-validated mean squared prediction error.  

Here is the LASSO approach on the **nyctaxi** data:

``` r
> lassout <- qeLASSO(nyctaxi,'tripTime') 
> lassout$lambda.whichmin
[1] 51
> lassout$whenEntered
  trip_distance PULocationID.132 PULocationID.186      DayOfWeek.1
               2               29               31               31
DOLocationID.132 DOLocationID.124      DayOfWeek.5  DOLocationID.33
              33               34               34               35
DOLocationID.189 DOLocationID.225 PULocationID.213  PULocationID.76
              35               35               37               38
PULocationID.263   DOLocationID.1 DOLocationID.177      DayOfWeek.2
              38               38               38               38
PULocationID.124  DOLocationID.35  DOLocationID.36  DOLocationID.89
              39               39               39               39
DOLocationID.231 DOLocationID.263 PULocationID.239 DOLocationID.155
              39               39               40               40
 PULocationID.40  PULocationID.43 PULocationID.146 PULocationID.161
              41               41               41               41
PULocationID.230  DOLocationID.22  DOLocationID.37 DOLocationID.161
              41               41               41               41
DOLocationID.162 DOLocationID.251 PULocationID.163 PULocationID.211
              41               41               42               42
PULocationID.257  DOLocationID.49  DOLocationID.81 DOLocationID.179
              42               42               42               42
...
```

For each value of &lambda; run by the code, one or more features joined the
model.  The cross-valued mean squared prediction error reached its
minimum at the 51st &lambda;.  Along the way, **trip_distance** was the
first feature added, at the 2nd &lambda;, followed 
by PULocationID132 (29th), PULocationID186 (31st), 
DayOfWeek.1 (31st) and so o.

## Methods based on measures of feature importance

Consider the Random Forests (RF) ML method.  It has various
hyperparameters, such as maximum number of levels in a tree, and we wish
to find a "good" combination of those settings.  As a byproduct, though,
the fact that at each level RF is entering a new feature, many RF
implemntations compute some kind of *variable importance* ranking--thus
giving us our desired ordered sequence.  

We can then run RF on each feature set in the sequence, or as mentioned,
try the sequence on some other ML method.  We would then use whichever
feature set yielded the best cross-validated performance.

Here is an example with the **nyctaxi** data:

``` r
> z <- qeRFranger(nyctaxi,'tripTime')
holdout set has  1000 rows
Loading required namespace: ranger
Warning message:
In eval(tmp, parent.frame()) :
  9 rows removed from test set, due to new factor levels
> z$variable.importance
trip_distance  PULocationID  DOLocationID     DayOfWeek
   2447411690     212569502     201650058      85733194
```

So, our feature set sequence might be:

<pre>
trip_distance
trip_distance and PULocationID
trip_distance and PULocationID and DOLocationID
trip_distance and PULocationID and DOLocationID
trip_distance and PULocationID and DOLocationID and DayOfWeek
</pre>

Note that the algorithm chose the categorical variables as a whole here,
not breaking down into their dummy variable components.

Another example is  Prncipal Components Analysis (PCA).  Here our p
features are replaced by p linear combinations (the "principle
components," PCs) of the original features.  The PCs are uncorrelated.
Note that this implies that the dataset is all numeric; you can use the
**regtools::factorsToDummies** to convert.  (The **regtools** package is
included by **qeML**.)

The PCs form our new features, and are ordered by variance, largest to
smallest.  Since a variable with small variance is essentially constant
and thus has little predictive value, we take only the PCs with larger
variance.  

For the first m PCs, say, we compare the sum of their variances 
(which is also the variance of their sum) to the total variance of the
response variable Y (i.e. the variable we are trying to predict).  By
varying the proportion, we produce an ordered sequence of feature sets,
as desired.

The **qeML** function **qePCA** first finds the PCs, then applies
whatever ML method is specified by the user.  The function **qeUMAP**
does the same for UMAP, a kind of nonlinear analog of PCA.

One more measure of variable importance is *Shapley values*.  This
concept describes an attempt to use game theory to apportion credit for
a win among several players in a team; the "players" here are the
features.  While it is a clever idea, it has been the subject of much
criticism in terms of practical value.

## Feature Ordering by Conditional Independence (FOCI)

This is my personal "go to" method for feature selection.  Its
theoretical foundations are complex, but it boils down to measuring the
reduction in variance in predicting Y from X, versus predicting Y simply
from its mean.  

FOCI is implemented in a CRAN package of the same name, and **qeML**'s
**qeFOCI** function provides a convenient interface.  Example:

``` r
> w <- qeFOCI(nyctaxi,'tripTime')
> w$selectedVar
   index            names
1:     1    trip_distance
2:   202  DOLocationID.75
3:   348      DayOfWeek.1
4:    78 PULocationID.151

```

Trip distance, dropoff location 75, Mondays and pickup location 151 were
chosen, in that order, after which the algorithm decided that adding any
further features was counterproductive.  Again, we could simply take
that 4-feature set for whatever ML method we wish, or we could use the
above to set up an ordered sequence of feature sets,

<pre>
trip_distance
trip_distance and DOLocationID.75
trip_distance and DOLocationID.75 and DayOfWeek.1
trip_distance and DOLocationID.75 and DayOfWeek.1 and PULocationID.151
</pre>

running our desired ML method on each feature set, then choosing the one
with best cross-validation performance.

## Direct dimension reduction for categorical data

Rather than doing feature selection *per se*, we might transform the
data to summary variables.  We could group the pickup/dropoff locations
by ZIP Code, for instance.  

Or, we could consider only the more frequently used locations.  The qeML
function **factorToTopLevels** would help here.  We will use it to
reduce pickup location to just a few places that appear a lot in our
data; all others will be lumped together as 'other'.

We invoke the function as follows:

``` r
> factorToTopLevels(nyctaxi$PULocationID,lowCountThresh=0)
```


![alt text](LowCountThresh.png)

Ah, yes, lots of locations have small counts, less than 50 and probably
many in the single-digits range.  So, **PULocationID** is a prime
candidate for simplication.  Let's go for a cutoff of 75.

``` r
> newPUlocs <- factorToTopLevels(nyctaxi$PULocationID,lowCountThresh=75)
```

So, now we have a new, simpler version of the pickup location data.
Let's take a look at old versus new.

``` r
> head(newPUlocs,50)
 [1] 236   238   100   161   236   161   238   107   170   other 170   137  
[13] 239   143   164   164   151   239   161   100   142   107   238   43   
[25] 237   170   238   238   236   161   236   other 230   138   other 79   
[37] 186   132   other 100   234   142   151   263   236   142   142   144  
[49] 236   230  
43 Levels: 100 107 113 114 13 132 137 138 140 141 142 143 144 148 151 ... other
> head(nyctaxi$PULocationID,50)
 [1] 236 238 100 161 236 161 238 107 170 211 170 137 239 143 164 164 151 239 161
[20] 100 142 107 238 43  237 170 238 238 236 161 236 41  230 138 261 79  186 132
[39] 152 100 234 142 151 263 236 142 142 144 236 230
224 Levels: 1 3 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ... 265
```

So for example pickup location in row 10 of our data was location 211,
but it was one of the "rare" locations, so it was recoded to "other".

How much dimension reduction did we accomplish?

``` r
> length(levels(newPUlocs))
[1] 43
```

That's much less than the original 266.  Of course, we could reduce even
further by using a larger threshold.

So, let's try out the new data:

``` r
> newDOlocs <- factorToTopLevels(nyctaxi$DOLocationID,lowCountThresh=75)
> nyctaxi$PULocationID <- newPUlocs
> nyctaxi$DOLocationID <- newDOlocs
> z <- qeLin(nyctaxi,'tripTime')
```

Dimension reduction at least got us a stable solution, no warning
messages.  Does it predict well?

``` r
> z$testAcc
[1] 240.5642
> z$baseAcc
[1] 430.4696
```

Mean Absolute Prediction Error is much less than what one would have by
simply guessing all cases to be the overall mean trip time.

By varying **lowCountThresh**, we could generate an ordered sequence of
feature sets, as before.

