

---
title: "PCA and UMAP"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{PCA and UMAP}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---


#  Clearing the Confusion: PCA and UMAP--How Are They Used in Prediction, and What about Claims That They Require Scaling?

Principal Components Analysis (PCA) is a well-established method of
dimension reduction.  It is often used as a means of gaining insight
into the "hidden meanings" in a dataset.  But in prediction
contexts--ML--it is mainly a technique for avoiding overfitting and excess
computation.

This tutorial has two goals:

* We will dispel the "abstract" reputation of PCA, by taking an up-close
  view in real data.

* We will take a critical look at the commonly-held view that one must
  center and scale one's data prior to performing PCA.`

A number of "nonlinear versons" of PCA have been developed, including 
Uniform Manifold Approximation and Projection (UMAP), which we will also
discuss briefly here.

# PCA

PCA forms new columns from the original columns of our data.  Each new
column is some linear combination of our original columns.  Let's see
concretely what that really means.

## Dataset

Consider **mlb**, a dataset included with **qeML**.  We will look at
heights, weights and ages of American professional baseball players.
(We will delete the first column, which records position played.)

``` r
> data(mlb)
> mlb <- mlb[,-1]
> head(mlb)
  Height Weight   Age
1     74    180 22.99
2     74    215 34.69
3     72    210 30.78
4     72    210 35.43
5     73    188 35.71
6     69    176 29.39
> mlb <- as.matrix(mlb)
> dim(mlb)
[1] 1015    3   # 1015 players, 3 measurements each
```

## Apply PCA

The standard PCA function in R is **prcomp**:

``` r
> z <- prcomp(mlb)  # defaults center=TRUE, scale.=FALSE)
> str(z)
List of 5
 $ sdev    : num [1:3] 20.87 4.29 1.91
 $ rotation: num [1:3, 1:3] -0.0593 -0.9978 -0.0308 -0.1101 -0.0241 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "Height" "Weight" "Age"
  .. ..$ : chr [1:3] "PC1" "PC2" "PC3"
 $ center  : Named num [1:3] 73.7 201.3 28.7
  ..- attr(*, "names")= chr [1:3] "Height" "Weight" "Age"
 $ scale   : logi FALSE
 $ x       : num [1:1015, 1:3] 21.46 -13.82 -8.6 -8.74 13.14 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:1015] "1" "2" "3" "4" ...
  .. ..$ : chr [1:3] "PC1" "PC2" "PC3"
 - attr(*, "class")= chr "prcomp"

```

What is in **z**?  Let's look at **rotation** first.

As you may know, each principal component (PC) is a linear combination
of the input features.  These coefficients are stored in **rotation**:

``` r
> head(pcaOut$rotation)
               PC1         PC2         PC3
Height -0.05934555 -0.11013610 -0.99214321
Weight -0.99776279 -0.02410352  0.06235738
Age    -0.03078194  0.99362420 -0.10845927

```
Though it may seem abstract, all that is happening is that *we started
with 3 variables*, Height, Weight and Age, and now *have created 3 new
variables*, **PC**, **PC2** and **PC3**.  

For instance,

PC2 = -0.11 Height - 0.02 Weight + 0.99 Age

Those 3 new variables are stored in the **x** component of **z**.

We originally 3 measurements on each of 1015, and now we have 3 new
measurements on each of those pepole:

``` r
> dim(z$x)
[1] 1015    3

``` r
> mlb[1,]
Height Weight    Age 
 74.00 180.00  22.99 
```

In the new data, his numbers are:

``` r
> pcaOut$x[1,]
      PC1       PC2       PC3 
21.458611 -5.201495 -1.018951 

```

Let's check!  Since the PCi are linear combinations of the original
columns, we can compute them via matrix multiplication:

``` r
> mlbc <- center(mlb)  # remember, prcomp did centering
> mlbc[1,] %*% z$rotation[,2]
          [,1]
[1,] -5.201495
```

Ah yes, it checks.

# Key properties

A key property of PCA is that the PCs

* (a) are arranged in order of decreasing variances, and

* (b) they are uncorrelated.

The variances (actually standard deviations) are reported in the return
object from **prcomp**:

``` r
> z$sdev
[1] 20.869206  4.288663  1.911677
```

Yes, (a) holds.  Let's double check, say for PC2:

``` r
> sd(z$x[,2])
[1] 4.288663
```

Yes.

What about (b)?

``` r
> cor(z$x)
              PC1           PC2          PC3
PC1  1.000000e+00 -1.295182e-16 2.318554e-15
PC2 -1.295182e-16  1.000000e+00 2.341867e-16
PC3  2.318554e-15  2.341867e-16 1.000000e+00
```

## Practical importance of (a) and (b)

Many data analysts, e.g. social scientists, use PCA to search for
patterns in the data.  In the ML context, though, our main interest is
prediction.  And if we have a large number of predictor variables, we
would like to reduce that number, in order to avoid avoid overfitting,
reduce computation and so on.  PCA can help us do that.

### Dataset

Here we will use another built-in dataset in **qeML**, a song database
named **fiftyksongs**.  It is a 50,000-row random subset of the famous Million
Song Dataset.

``` r
> dim(fiftyksongs)
[1] 50000    91

> w <- prcomp(fiftyksongs[,-1])  # first column is "Y", to be predicted
> w$sdev
 [1] 2127.234604 1168.717654  939.840843  698.575319  546.683262  464.683454
 [7]  409.785038  395.928095  380.594444  349.489142  333.322277  302.017413
[13]  282.819445  260.362550  255.472674  248.401464  235.939740  231.404983
[19]  220.682026  194.828458  193.645669  189.074051  187.455170  180.727969
[25]  173.956554  166.733909  156.612298  151.194556  144.547790  138.820897
[31]  133.966493  124.514162  122.785528  115.486330  112.819657  110.379903
[37]  109.347994  106.551231  104.787668   99.726851   99.510556   97.599960
[43]   93.161508   88.559160   87.453436   86.870468   82.452985   80.058511
[49]   79.177031   75.105451   72.542646   67.696172   64.079955   63.601079
[55]   61.105579   60.104226   56.656737   53.166604   52.150838   50.515730
[61]   47.954210   47.406341   44.272814   39.914361   39.536682   38.653450
[67]   37.228741   36.007748   34.192456   29.523751   29.085855   28.387604
[73]   26.325406   24.763188   22.192984   20.203667   19.739706   18.453111
[79]   14.238237   13.935897   10.813426    9.659868    8.938295    7.725284
[85]    6.935969    6.306459    4.931680    3.433850    3.041469    1.892496

> 
```

The first column of the data set is the year of release of the song,
while the other 90 are various audio measurements.  The goal is to
predict the year.  

One hopes to substantially reduce the number of predictors from 90.  To
this aim, we might consider retaining only the first few PCs, i.e. only
the first few columns in the 'x' component returned by **prcomp**.  Note
the following:

1.  The set of all 90 PCs captures the entire linear variability of
    the predictor variables.

2.  The PCs are ordered in decreasing variance, and the last few
    variances tend to be very small relative to the earlier ones. Such
    PCs are essentially constant, thus useless as predictors.  (And due
    to sample variance, the coefficients for those last PCs probably
    would be unreliable anyway.)

3.  So it makes sense to retain only the first m PCs.  But what should
    set m to?  We would like it to be small, but if it is too small,
    we may miss important predictive information.

4.  Note that since the PCs are uncorrelated, then roughly speaking,
    the predictive information added in using m+1 PCs instead of m
    will not overlap that of the previous m PCs.  Again, though, it's
    a matter of where to stop, the famous Bias-Variance Tradeoff;
    increasing m reduces bias but increases variance of predicted
    values.

5.  We try various values of m, assessing each via cross-validations.

Let's try the latter, with a linear model.  We'll first use the first
PC, then the first two, then the first three etc.  We'll do 5 runs at
each level.

``` r
> f50s <- fiftyksongs
> f50s[,-1] <- w$x  # replace orig. features by PCs
> dim(f50s)  
[1] 50000    91

> xval <- function(i)
{
   d <- f50s[,1:(1+i)]
   print( replicMeans(10,"qeLin(d,'V1')$testAcc") )
}

> for (i in 1:25) xval(i)
[1] 8.079996
[1] 8.067937
[1] 7.925185
...
[1] 7.749934
[1] 7.740841
[1] 7.604057
```

It seems to m = 25 did best, and larger values may do even better.  

Also, of course, some other ML method than a linear model may do
substantially better.  The above is just a start.



## The Scaling Issue

### Overview

The recommendation to scale is common.  Here are some examples: 

* R **prcomp()** man page

    They say "scaling is advisable":

> scale.: a logical value indicating whether the variables should be
>           scaled to have unit variance before the analysis takes place.
>           The default is ‘FALSE’ for consistency with S, but in general
>           scaling is advisable.

* [Scikit-Learn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html):

    Actually, the mention of normal distributions is misleading and in
    any case not relevant, but again there is a rather imperative
    statement to scale:

> Feature scaling through standardization (or Z-score normalization) can
> be an important preprocessing step for many machine learning algorithms.
> Standardization involves rescaling the features such that they have the
> properties of a standard normal distribution with a mean of zero and a
> standard deviation of one.

* [DataCamp](https://www.datacamp.com/community/tutorials/pca-analysis-r)

Again, their phrasing is rather imperative:

> Note that the units used [in the **mtcars** dataset] vary and occupy
> different scales...You will also set two arguments, center and scale, to
> be TRUE. 

* [caret](https://cran.r-project.org/package=caret), **preProcess** man
  page

    Scaling done unless you say no:

> If PCA is requested but centering and scaling are not, the values will
> still be centered and scaled. 

* [Visually Enforced](https://www.gastonsanchez.com/visually-enforced/how-to/2012/06/17/PCA-in-R/)

The word "must" is used here:

> Since most of the times the variables are measured in different scales,
> the PCA must be performed with standardized data (mean = 0, variance =
> 1).

## The perceived problem

As the DataCamp statement notes, some data may be "large" while other
data are "small."  There is a concern that, without scaling, the large
ones will artificially dominate.  This is especially an issue in light
of the variation in measurement systems -- should a variable measured in
kilometers be given more weight than one measured in miles?

## Motivating counterexample

Consider a setting with two independent variables, A and B, with means
100, and with Var(A) = 500 and Var(B) = 2.  Let A' and B' denote these
variables after centering and scaling.

PCA is all about removing variables with small variance, as they are
essentially constant.  If we work with A and B, we would of course use
only A.  **But if we work with A' and B', we would use both of them, as
they both have variance 1.0.**  Scaling has seriously misled us here.

## Alternatives

The real goal should be to make the variables *commensurate*.
Standardizing to mean 0, variance 1 is not the only way one can do this.
Consider the following alternatives.

* Do nothing.  In many data sets, the variables of interest are already
  commensurate.  Consider survey data, say, with each survey question
asking for a response on a scale of 1 to 5.  No need to transform the
data here, and worse, standardizing would have the distortionary effect
of exaggerating rare values in items with small variance.

* Map each variable to the interval [0,1], i.e. t -> (t-m)/(M-m), where
  m and M are the minimum and maximum values of the given variable.
  This is typically better than standardizing, but it does have some
  problems.  First, it is sensitive to outliers.  This might be
  ameliorated with a modified form of the transformation (and ordinary
  PCA has the same problem), but a second problem is that new data --
  new data in prediction applications, say -- may stray from this [0,1]
  world.

* Instead of changing the *standard deviation* of a variable to 1.0,
  change its *mean* to 1.0.  This addresses the miles-vs.-kilometers
concern more directly, without inducing the distortions I described
above.  And if one is worried about outliers, then divide the variable
by the median or other trimmed mean.


