<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Introduction to ML</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Introduction to ML</h1>
</header>
<h1 id="the-10-page-machine-learning-book">The 10-Page Machine Learning Book</h1>
<p>(The title here alludes to Andriy Burkov’s excellent work, <em>The Hundred-Page Machine Learning Book</em>. Note too my own forthcoming book, <em>The Art of Machine Learning: Algorithms+Data+R</em>.)</p>
<p>Here we give an overview of the most widely used predictive methods in statistical/machine learning (ML). For each one, we present</p>
<ul>
<li><p>background</p></li>
<li><p>overview of how it works</p></li>
<li><p>function in the R package, <strong>qeML</strong> (readers without R background or who simply wish to acquire an overview of ML may skip the R code without loss of comprehehnsion of the text).</p></li>
</ul>
<p>We’ll also discuss the overfitting issue.</p>
<h2 id="notation">Notation</h2>
<p>For convenience, we’ll let Y denote the variable to be predicted, i.e. the response variable, and let X denote the set of predictor variables/features. (ML people tend to use the term <em>features</em>, while In stat, the term <em>predictors</em> is common. In applied fields, e.g. economics or psychology, some use the term <em>independent variables</em>.)</p>
<p>We develop our prediction rule from available <em>training data</em>, consisting of n data points, denoted by (X<sub>1</sub>, Y<sub>1</sub>),.., (X<sub>n</sub>, Y<sub>n</sub>). We wish to predict new cases (X<sub>new</sub>,Y<sub>new</sub>) in the future, in which X<sub>new</sub> is known but Y<sub>new</sub> needs to be predicted.</p>
<p>So, we may wish to predict human weight Y from height X, or from height and weight in the 2-component vector X.<br />
Say we have the latter situation, and data on n = 100 people. Then for instance X<sub>23</sub> would be the vector of height and age for the 23rd person in our training data, and Y<sub>23</sub> would be that person’s weight.</p>
<p>The vector X may include <em>indicator</em> variables, which have values only 1 or 0. We may for instance predict weight from height, age and gender, the latter being 1 for female, 0 for male.</p>
<p>If Y represents a binary variable, we represent it as an indicator variable. In the famous Pima Diabetes dataset in the <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning Repository</a>, 1 means diabetic, 0 means not.</p>
<p>If Y is categorical, we represent it by several indicator variables, one for each category. In another disease-related UCI dataset (to be discussed below), Y is status of a person’s vertebrae condition; there are 2 disease types, and normal, for a total of 3 categories. Y = (0,0,1) for a normal person, for instance. Thus Y can be a vector too.</p>
<h2 id="running-example">Running example</h2>
<p>The package’s built-in dataset <strong>mlb</strong> consists of data on major league baseball players (courtesy of the UCLA Dept. of Statistics).</p>
<p>Here is a glimpse of the data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">data</span>(mlb)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">head</span>(mlb)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>             Name Team       Position Height Weight   Age PosCategory</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>   Adam_Donachie  BAL        Catcher     <span class="dv">74</span>    <span class="dv">180</span> <span class="fl">22.99</span>     Catcher</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>       Paul_Bako  BAL        Catcher     <span class="dv">74</span>    <span class="dv">215</span> <span class="fl">34.69</span>     Catcher</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> Ramon_Hernandez  BAL        Catcher     <span class="dv">72</span>    <span class="dv">210</span> <span class="fl">30.78</span>     Catcher</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>    Kevin_Millar  BAL  First_Baseman     <span class="dv">72</span>    <span class="dv">210</span> <span class="fl">35.43</span>   Infielder</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>     Chris_Gomez  BAL  First_Baseman     <span class="dv">73</span>    <span class="dv">188</span> <span class="fl">35.71</span>   Infielder</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>   Brian_Roberts  BAL Second_Baseman     <span class="dv">69</span>    <span class="dv">176</span> <span class="fl">29.39</span>   Infielder</span></code></pre></div>
<h2 id="the-r-packages-qe-series-functions">The R package’s <strong>qe</strong>*-series functions</h2>
<p>Here “qe” stands for <strong>“quick and easy.”</strong> The functions have a simple, uniform interface, and most importantly, <strong>require no setup.</strong> To fit an SVM model, say, one simply calls <strong>qeSVM()</strong>, no preparation calls to define the model etc.</p>
<p>The call form is</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model fit <span class="ot">&lt;-</span> qe<span class="sc">&lt;</span>model name<span class="sc">&gt;</span>(<span class="sc">&lt;</span>data name<span class="sc">&gt;</span>,<span class="sc">&lt;</span>Y name<span class="sc">&gt;</span>)</span></code></pre></div>
<p>As noted, no prior calls are needed to define the model, etc.</p>
<h2 id="example">Example</h2>
<p>Let’s predict weight from height and age, using two methods, k-Nearest Neighbor and random forests.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mlb <span class="ot">&lt;-</span> mlb[,<span class="dv">4</span><span class="sc">:</span><span class="dv">6</span>]  <span class="co"># columns for height, weight and age</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>knnout <span class="ot">&lt;-</span> <span class="fu">qeKNN</span>(mlb,<span class="st">&#39;Weight&#39;</span>)  <span class="co"># fit k-Nearest Neighbor model</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>rfout <span class="ot">&lt;-</span> <span class="fu">qeRF</span>(mlb,<span class="st">&#39;Weight&#39;</span>)  <span class="co"># fit random forests model</span></span></code></pre></div>
<p>Default values of hyperparameters are used but can be overridden. Prediction of new cases is equally easy, in the form</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(<span class="sc">&lt;</span>model fit<span class="sc">&gt;</span>, <span class="sc">&lt;</span>new X value<span class="sc">&gt;</span>)</span></code></pre></div>
<p>E.g. to predict the weight of a new player of height 70 and age 28, run</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">predict</span>(rfout,<span class="fu">c</span>(<span class="dv">70</span>,<span class="dv">28</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>       <span class="dv">2</span> </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fl">184.1626</span> </span></code></pre></div>
<p>The model is automatically assessed on a holdout set (the concept is discussed later in this document):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> rfout<span class="sc">$</span>testAcc  <span class="co"># mean absolute prediction error</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="fl">15.16911</span></span></code></pre></div>
<p>(The formation of a holdout set can be suppressed.)</p>
<h2 id="regression-and-classification-problems-regression-functions">Regression and classification problems, regression functions</h2>
<p>Prediction applications in which Y is a continuous variable, say weight, or at least ordinal, are called <em>regression settings</em>. Applications in which Y is categorical, i.e. Y is a factor variable in R, say predicting the player’s position (e.g. pitcher) are <em>classification settings</em>.</p>
<p>Somewhat confusingly, both settings make use of the <em>regression function</em>, m(t) = E(Y | X = t), the mean value of Y in the subpopulation defined by X = t. If say we are predicting weight in the <strong>mlb</strong> data, then for instance m(71,23) would be the mean weight among all players of height 71 inches and 23 years old. To predict the weight of a new player, say height 77 and age 19, we use m(77,19).</p>
<p>In classification problems, Y is converted to a set of indicator variables. For the position ‘pitcher’ in the <strong>mlb</strong> data, we would have Y = 1 or 0, depending on whether the player is a pitcher or not. (Position is in column 3 of the original dataset.) Then E(Y | X = t) reduces to P(Y = 1 | X = t), the probability that the player is a pitcher given the player’s height, weight and age, say. We would typically find probabilities for each position, then guess the one with the highest probability.</p>
<p>In other words, the regression function m(t) is central to both regression and classification settings. The statistical/machine learning methods presented here amount to ways to estimate m(t). The methods are presented below in an order that shows connection between them.</p>
<p>Even though each ML method has its own special <em>tuning parameters</em> or <em>hyperparameters</em>, used to fine-tune performance, they all center around the regression function m(t).</p>
<p>The <strong>qe</strong> series function sense whether the user is specifying a regression setting or a classification setting, by noting whether the Y variable (second argument) is numeric or an R factor.</p>
<h2 id="ml-predictive-methods">ML predictive methods</h2>
<p>We now present the “30,000 foot” view of the major statistical/machine learning methods.</p>
<h3 id="k-nearest-neighbors">k-Nearest Neighbors</h3>
<p>This method was originally developed by statisticians, starting in the 1950s and 60s.</p>
<p>It’s very intuitive. To predict, say, the weight of a new player of height 72 and age 25, we find the k closest players in our training data to (72,25), and average their weights. This is our estimate of m(72,25), and we use it as our prediction.</p>
<p>The <strong>qeKNN()</strong> function wraps <strong>kNN()</strong> in <strong>regtools</strong>. The main hyperparameter is the number of neighbors k. As with any hyperparameter, the user aims to set a “Goldilocks” level, not too big, not too small. Setting k too small will result in our taking the average of just a few Y values, too small a sample. On the other hand, too large a value for k will some distant data points may be used that are not representative.</p>
<h3 id="random-forests">Random forests</h3>
<p>This method was developed mainly by statisticians, starting in the 1980s.</p>
<p>This is a natural extension of k-NN, in that it too creates a neighborhood and averages Y values within the neighborhood. However, it does so in a different way, creating tree structures.</p>
<p>Say we are predicting blood pressure from height, weight and age. We first ask whether the height is above or below a certain threshold. After that, we ask whether weight is above or below a certain (different) threshold. This partitions height-weight space into 4 sectors. We then might subdivide each sector according to whether age is above or below a threshold, now creating 8 sectors of height-weight-age space. Each sector is now a “neighborhood.” To predict a new case, we see which neighborhood it belongs to, then take our prediction to be the average Y value among training set points in that neighborhood.</p>
<p>The word <em>might</em> in the above paragraph alludes to the fact that the process may stop early, if the current subdivision is judged fine enough to produce good accuracy. And one generally wants to avoid having neighborhoods (<em>nodes</em> in the tree) that don’t have many data points; this is controlled by a hyperparameter, say setting a minimum number of data points per node; if a split would violate that rule, then don’t split. Of course, if we set out threshold too high, we won’t do enough splits, so again we need to try to find a “Goldilocks” level.</p>
<p>Here is an example using the UCI Vertebrae dataset. There are 6 predictor variables, named V1 through V6 consisting of various bone measurements. There are 3 classes, DH (Disk Hernia), Spondylolisthesis (SL), Normal (NO). A single tree is shown.</p>
<figure>
<img src="RpartVert.png" alt="alt text" /><figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>At the root, if the predictor variable V6 in our new case to be predicted is &lt; 16, we go left, otherwise right. Say we go left. Then if V4 &lt; 28 in the new case, we go left again, getting to a leaf, in which we guess DH. The 0.74 etc. mean that for the training data that happen to fall into that leaf, 74% of them are in class DH, 26% are NO and 0% are SL. So we gues DH.</p>
<p>Clearly, the order in which the predictor variables are evaluated (e.g. height, weight and age in the <strong>mlb</strong> data) can matter a lot. So, more than one tree is constructed, with random orders. The number of trees is another hyperparameter. Each tree gives us a prediction for the unknown Y. In a regression setting, those predictions are averaged to get our final prediction. In a classification setting, we see which class was predicted most often among all those trees.</p>
<p>The thresholds used at each node are determined through a complicated process, depending on which implemented of RF one uses.</p>
<p>The <strong>qeRF()</strong> function wraps the function of the same name in the <strong>randomForests</strong> package.</p>
<h3 id="boosting">Boosting</h3>
<p>This method has been developed both by CS and statistics people. The latter have been involved mainly in <em>gradient</em> boosting, the technique used here.</p>
<p>The basic idea is to iteratively build up a sequence of trees, each of which is focused on the data points on which the last predicted poorly. At the end, all the trees are combined.</p>
<p>The <strong>qeGBoost()</strong> wraps <strong>gbm()</strong> in the package of the same name. It is tree-based, with hyperparameters similar to the random forests case, plus a <em>learning rate</em>. The latter controls the size of iteration steps.</p>
<h3 id="linear-model">Linear model</h3>
<p>This of course is the classical linear regression model, invented 200 years ago (!) and developed by statisticians.</p>
<p>For motivation, here is a graph of mean weight vs. height for the <strong>mlb</strong> data:</p>
<figure>
<img src="WtVsHt.png" alt="alt text" /><figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>The means seem to lie near a straight line. (Remember, though, that these are sample means.) That suggests modeling m(t) is a linear function.</p>
<p>For example, a model for mean weight, given height and age, would be</p>
<p>m(height,age) = β<sub>0</sub> + β<sub>1</sub> height + β<sub>2</sub> age</p>
<p>for unknown population constants β<sub>i</sub>, which are estimated from our training data, using the classic <em>least-squares</em> approach. Our estimates of the β<sub>i</sub>, denoted b<sub>i</sub>, are calculated by minimizing</p>
<p>Σ<sub>i</sub> [ weight<sub>i</sub> - (b<sub>0</sub>+b<sub>1</sub>height<sub>i</sub>+b<sub>2</sub>age<sub>i</sub>)] <sup>2</sup></p>
<p>This is a simple calculus problem. We find the partial derivatives of the sum of squares with respect to the b<sub>i</sub>, and set them to 0. This gives us 3 equations in 3 unknowns, and since these equations are linear, it is easy to solve them for the b<sub>i</sub>.</p>
<p>There are no hyperparameters here.</p>
<p>This model is mainly for regression settings, though some analysts use it in classification. If used in conjunction with polynomials (see below), this may work as well or better than the logistic model (see below).</p>
<p>The function <strong>qeLin()</strong> wraps the ordinary <strong>lm()</strong>. It mainly just calls the latter, but does some little fixess.</p>
<h3 id="logistic-model">Logistic model</h3>
<p>This is a generalization of the linear model, developed by statisticians and economists.</p>
<p>This model is only for classification settings. Since m(t) is now a probability, we need it to have values in the interval [0,1]. This is achieved by feeding a linear model into the <em>logistic function</em>, l(u) = (1 + exp(-u))<sup>-1</sup>. So for instance, to predict whether a player is a catcher (Y = 1 if yes, Y = 0 if no), Again, there are no hyperparameters here.</p>
<p>P(catcher | height, weight, age) = m(height,weight,age) = 1 / [1 + exp{-(β<sub>0</sub> + β<sub>1</sub> height + β<sub>2</sub> weight + β<sub>3</sub> age)}]</p>
<p>The β<sub>i</sub> are estimated from the sample data, using a technique called <em>iteratively reweighted least squares</em>.</p>
<p>The function <strong>qeLogit()</strong> wraps the ordinary R function <strong>glm()</strong>, but adds an important feature: <strong>glm()</strong> only handles the 2-class setting, e.g. catcher vs. non-catcher. The <strong>qeLlogit()</strong> handles the c-class situation by calling <strong>glm()</strong> one class at a time, generating c <strong>glm()</strong> outputs. When a new case is to be predicted, it is fed into each of the c <strong>glm()</strong> outputs, yielding c probabilities. It then predicts the new case as whichever class has the highest probability.</p>
<h3 id="polynomial-linear-models">Polynomial-linear models</h3>
<p>Some people tend to shrink when they become older. Thus we may wish to model a tendency for people to gain weight in middle age but then lose weight as seniors, say</p>
<p>m(height,age) = β<sub>0</sub> + β<sub>1</sub> height + β<sub>2</sub> age + β<sub>3</sub> age<sup>2</sup></p>
<p>where presumably β<sub>3</sub> &lt; 0.</p>
<p>We may even include a height X age product term, allowing for interactions. Polynomials of degree 3 and so on could also be considered. The choice of degree is a hyperparameter.</p>
<p>This would seem nonlinear, but that would be true only in the sense of being nonlinear in age. It is still linear in the β<sub>i</sub> – e.g. if we double each β<sub>i</sub> in the above expression, the value of the expression is doubled – so <strong>qeLin()</strong> can be used, or <strong>qeLogit()</strong> for classification settings.</p>
<p>Forming the polynomial terms by hand would be tedious, especially since we would also have to do this for predicting new cases. Instead, we use <strong>qePolyLin()</strong> (regression setting) and <strong>qePolyLog()</strong> (classification). They make use of the package <strong>polyreg</strong>.</p>
<p>Polynomial models can in many applications hold their own with the fancy ML methods. One must be careful, though, about overfitting, just as with any ML method.</p>
<h3 id="the-lasso">The LASSO</h3>
<p>Some deep mathematical theory implies that in linear models it may be advantageous to shrink the estimated b<sub>i</sub>. The LASSO method does this in a mathematically rigorous manner. The LASSO is especially popular as a tool for predictor variable selection (see below).</p>
<p>The function <strong>qeLASSO()</strong> wraps <strong>cvglmnet()</strong> in the <strong>glmnet</strong> package. The main hyperparameter controls the amount of shrinkage.</p>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<p>These were developed originally in the AI community, and later attracted interest among statisticians. They are used mainly in classification settings.</p>
<p>Say in the baseball data we are predicting catcher vs. non-catcher, based on height and weight. We might plot a scatter diagram, with height on the horizontal axis and weight on the vertical, using red dots for the catchers and blue dots for the non-catchers. We might draw a line that best separates the red and blue dots, then predict new cases by observing which side of the line they fall on. This is what SVM does (with more predictors, the line become a plane or hyperplane).</p>
<p>Here is an example, using the Iris dataset built in to R:</p>
<figure>
<img src="SVM.png" alt="alt text" /><figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>There are 3 classes, but we are just predicting setosa species (shown by + symbols) vs. non-setosa (shown by boxes) here. Below the solid line, we predict setosa, otherwise non-setosa.</p>
<p>SVM philosophy is that we’d like a wide buffer separating the classes, called the <em>margin</em>, denoted by the dashed lines. Data points lying on the edge of the margin are termed <em>support vectors</em>, so called because if any other data point were to change, the margin would not change.</p>
<p>In most cases, the two classes are not linearly separable. So we allow curved boundaries, implemented through polynomial (or similar) transformations to the data. The degree of the polynomial is a hyperparameter, named <strong>gamma</strong> here.</p>
<p>Another hyperparameter is <strong>cost:</strong> Here we allow some data points to be within the margin. The cost variable is roughly saying how many exceptions we are willing to accept.</p>
<p>The <strong>qeSVM()</strong> function wraps <strong>svm()</strong> in the <strong>e1071</strong> package.</p>
<h3 id="neural-networks">Neural networks</h3>
<p>These were developed almost exclusively in the AI community.</p>
<p>An NN consists of <em>layers</em>, each of which consists of a number of <em>neurons</em> (also called <em>units</em> or <em>nodes</em>). Say for concreteness we have 10 neurons per layer. The output of the first layer will be 10 linear combinations of the predictor variables/features, essentially 10 linear regression models. Those will be fed into the second layer, yielding 10 “linear combinations of linear combinations,” and so on.</p>
<p>In regression settings, the outputs of the last layer will be averaged together to produce our estimated m(t). In the classification case with c classes, our final layer will have c outputs; whichever is largest will be our predicted class.</p>
<p>“Yes,” you say, “but linear combinations of linear combinations are still linear combinations. We might as well just use linear regression in the first place.” True, which is why there is more to the story: <em>activation functions</em>. Each output of a layer is fed into a function A(t) for the purpose of allowing nonlinear effects in our model of m(t).</p>
<p>For instance, say we take A(t) = t^2 (not a common choice in practice, but a simple one to explain the issues). The output of the first layer will be quadratic functions of our features. Since we square again at the outputs of the second layer, the result will be 4th-degree polynomials in the features. Then 8th-degree polynomials come out of the third layer, and so on.</p>
<p>One common choice for A(t) is the logistic function l(u) we saw earlier. Another popular choice is ReLU, r(t) = max(0,t). No matter what we choose for A(t), the point is that we have set up a nonlinear model for m(t).</p>
<p>Here’s an example, again with the Vertebrae data: The predictor variables V1, V2 etc. for a new case to be predicted enter on the left, and the predictions come out the right; whichever of the 3 outputs is largest, that will be our predicted class.</p>
<figure>
<img src="VertebraeNN.png" alt="alt text" /><figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>The first layer consists of V1 through V6. The second layer, our only <em>hidden</em> layer here, has three neurons. Entering on the left of each neuron is a linear combination of V1 through V6. The outputs are fed into A(t) and then to the third layer.</p>
<p>Hyperparameters for NNs include the number of layers, the number of units per layer (which need not be the same in each layer), and the activation function.</p>
<p>The linear combination coefficients, shown as numeric labels in the picture, are known as <em>weights</em>. (They are also called <em>parameters</em>, not to be confused with hyperparameters.) How are they calculated? Again least squares is used, minimizing</p>
<p>Σ<sub>i</sub> (Y<sub>i</sub> - finaloutput<sub>i</sub>) <sup>2</sup></p>
<p>Let n<sub>w</sub> denote the number of weights. This can be quite large, even in the millions. Moreover, the n<sub>w</sub> equations we get by setting the partial derivatives to 0 are not linear.</p>
<p>Thus this is no longer a “simple” calculus problem. Iterative methods must be used, and it can be extremely tricky to get them to converge. Here’s why:</p>
<p>Though far more complex than in the linear case, we are still in the calculus realm. We compute the partial derivatives of the sum of squares with respect to the n<sub>w</sub> weights, and set the results to 0s. So, we are finding roots of a very complicated function in n<sub>w</sub> dimensions, and we need to do so iteratively.</p>
<p>A simplified version of the iteration process is as follows. Consider the function f() graphed below:</p>
<figure>
<img src="ObjFtnPlusTangent.png" alt="alt text" /><figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>There is an overall minimum at approximately x = 2.2. This is termed the <em>global minimum</em>. But there is also a <em>local minimum</em>, at about x = 0.4; that term means that this is the minimum value of the function only for points near—“local to”— 0.4. Let’s give the name x<sub>0</sub> to the value of x at the global minimum.</p>
<p>Denote our guess for x<sub>0</sub> at iteration i by g<sub>i</sub>. Say our initial guess g<sub>0</sub> = 1.1.</p>
<p>The tangent line is pointing upward to the right, i.e. has positive slope, so it tells us that by going to the left we will go to smaller values of the function. We do want smaller values, but in this case, the tangent is misleading us. We should be going to the right, towards 2.2, where the global minimum is.</p>
<p>You can see that if our current guess were near 2.2, the tangent line would guide us in the right direction. But we see above that it can send us in the wrong direction. One remedy (among several typically used in concert) is to not move very far in the direction the tangent line sends us. The idea behind this is, if we are going to move in the wrong direction, let’s limit our loss. The amount we move is called the <em>step size</em> in general math, but the preferred ML term is the <em>learning rate</em>. And, this is yet another hyperparameter.</p>
<p>So, NNs are arguably the most complex of all the methods described here, and tend to use huge amounts of computing time, even weeks!</p>
<p>The <strong>qeNeural()</strong> function allows specifying the numbers of layers and neurons per layer, and the number of iterations. It wraps <strong>krsFit()</strong> from the <strong>regtools</strong> package, which in turn wraps the R <strong>keras</strong> package (and there are further wraps involved after that).</p>
<h2 id="overfitting">Overfitting</h2>
<p>Up to a point, the more complex a model is, the greater its predictive power. “More complex” can mean adding more predictor variables, using a higher-degree polynomial, adding more layers etc.</p>
<p>As we add more and more complexity, the <em>model bias</em> will decrease, meaning that our models become closer to the actual m(t), in principle. But the problem is that at the same time, the <em>variance</em> of a predicted Y<sub>new</sub> is increasing.</p>
<p>Say again we are predicting human weight from height, with polynomial models. With a linear model, we use our training data to estimate two coefficients, b<sub>0</sub> and b<sub>1</sub>. With a quadratic model, we estimate three, and estimate four in the case of a cubic model and so on. But it’s same training data in each case, and intuitively we are “spreading the data thinner” with each more complex model. As a result, the standard error of our predicted Y<sub>new</sub> (estimated standard deviation) increases.</p>
<p>Hence the famous Bias-Variance Tradeoff. If we use too complex a model, the increased variance overwhelms the reduction in bias, and our predictive ability suffers. We say we have <em>overfit</em>.</p>
<p>So there is indeed a “Goldilocks” level of complexity, an optimal polynomial degree, optimal number of nearest neighbors, optimal <em>network architecture</em> (configuration of layers and neurons), and so on. How do we find it?</p>
<p>Alas, there are no magic answers here. The general approach is <em>cross-validation</em>. Here is a simple version: We set aside part of our training data as a <em>holdout set</em>; the remainder becomes our new training data. For each of our candidate models, e.g. each polynomial degree, we fit the model to the training set and then use it to predict the holdout data. We then choose the model that does best on the holdout data.</p>
<p>All the <strong>qe</strong> functions allow one to randomly partition the data, treating one part as the training set, and using the rest, the <em>holdout</em> set, to treat as new data to get a reliable estimate of the performance of the model; the function fits on the training set, then uses the result to predict the holdout set. Though a model may predict well on the training set, it may do less well on the holdout data, indicating overfitting.</p>
<p>Among other things, this means that if our training data have a very large number of predictor variables, we may wish to delete some, or possibly combine them, in order to reduce complexity.</p>
<p>One way to combine them is <em>Principal Components Analysis</em> (PCA). One creates new predictor variables as linear combinations of the original ones, then retains only the combinations that have a large variance. As you may have guessed by now, “There’s an app for that” – <strong>qePCA()</strong>.</p>
<p>Many people use the LASSO for predictor variable selection. Since typically most of the LASSO coefficients are 0s, we use only the variables with nonzero coefficients. One would then predict with that simplified LASSO model.</p>
<h2 id="which-one-to-use">Which one to use?</h2>
<p>With many datasets, all the above ML methods will give similar results. But for some other datasets, it really makes a difference which one we use.</p>
<p>So, we should try all of methods of interest with holdout data, then compare. Well, “there’s an app” for this too, <strong>qeCompare()</strong>.</p>
</body>
</html>
