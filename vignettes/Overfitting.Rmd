
---
title: "Overfitting"
output: 
   tufte::tufte_html: default
vignette: >
  %\VignetteIndexEntry{Overfitting}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Clearing the Confusion:  a Closer Look at Overfitting

The concept of overfitting is often mentioned in prediction
applications, but often in vague terms.  Let's see what is really
occurring.

# Preparation

## Goals

Explanations of overfitting in machine learning tend to be frustratingly
vague.  We for instance hear "An overfitted model predicts poorly on new
examples," which is a definition, not an explanation.  Or we hear that
overfitting results from "fitting the training data too well" or that we
are "fitting the noise," again rather unsatisfying.

Somewhat better--but only somewhat-- are the explanations based on the
famous Bias-Variance Tradeoff:

> Mean squared prediction error (MSPE) is the sum of squared bias and variance.
> Bias and variance are at odds with each other.  The lower the bias, the
> more the variance.  As we move through a sequence of models of
> increasing complexity, eventually the reduction in bias is overwhlemed
> by the increase in variance, a net loss.  Somewhere in that sequence
> of models is a best one, i.e. one with minimum MSPE.

OK, but the bias and variance of WHAT?  Our treatment here is based on
that notion, **with emphasis on what it is that we are really
talking about** in discussing bias and variance.

## Required Background

Only very basic statistical concepts are needed, plus patience (document
is a bit long).  Expected value notation, E(), is used in a couple of
places, but is explained intuitively in the given context, and is not
vital to the exposition.

## Setting

Let Y denote our outcome variable, and X represent the vector of our
predictors/features.  We are predicting Y from X.  For instance, Y might
be human weight, with X being (height, age).  We include binary
```{marginfigure}
In multiclass settings with c classes, Y is
a vector of c 1s and 0s, with only one component being 1 at a time.
```
classification problems, with Y = 1 or 0, coding the "yes" class and
"no" class.  


Overfitting is often described as arising when we go too far towards the
low-bias end of the Bias-Variance Tradeoff.  Yes, but what does that
really mean in the prediction context?

</br>
</br>

# The "true" relation between Y and X

The *regression function of Y on X*, &rho;(t), is defined to be the mean
```{marginfigure}
Though some books use the term "regression" to mean a linear model, the
actual definition is unrestricted; it applies just as much to, say,
random forests as to linear models.
```
Y for the given X values t.  In the weight/height/age example,
&rho;(68.2,32.9) is the mean weight of all people of height 68.2 and age
32.9.   

The &rho; function is the best (i.e. minimum MSPE) predictor of weight
based on height and age, the ideal.  Note the phrase "based on," though.
If we had a third predictor variable/feature available, say waist size,
there would be another &rho; function for that 3-predictor setting,
better than the 2-predictor one.

Note too that the &rho; function is a population entity, which we estimate
from our sample data.  Denote the estimated &rho;(t) by r(t).  Say we
are studying diabetic people.  &rho;(t) gives us the relation of weight
vs.  height and age in the entire population of diabetics; if our study
```{marginfigure}
Note:  "Sample" will mean our entire dataset; we will say, "A sample of 100
people," as in statistics courses, not "We have 100 samples," the
machine learning term.
```
involves 100 patients, that is considered a sample from that population,
and our estimate r(t) is based on that sample.


The term *population* is used in the statistics community.  Those who
come to ML from the computer science world tend to use terms like
*probabilistic generating process*.  Either way, it is crucial to keep
in mind that we treat our data as randomly generated.  If our data
consists of 100 diabetes patients, we treat them as being randomly drawn
from the conceptual population of all diabetics.

In a binary classification problem, the regression function reduces to the
probability of class 1, since the mean of a 0,1 variable is the
probability of a 1.

## Example: estimating the &rho; function via a linear model 

Say we use a linear model to predict weight from height, i.e. our model
for &rho;(height) is

&rho;(weight) = mean weight = &beta;<sub>0</sub> + &beta;<sub>1</sub> height

This is called a "parametric" model, in that it expresses &rho;(t) in terms
of some parameters, in this case the &beta;<sub>i</sub>.

Our sample-based estimate of the function &rho;(t) is a function r(t),
with

r(height) = mean weight = b<sub>0</sub> + b<sub>1</sub> height

The b<sub>i</sub>, say computed using the familiar least-squares method,
are the sample estimates of the &beta;<sub>i</sub>.

## Example: estimating the &rho; function via a k-NN model 

It will be convenient here to use as our nonparametric method the
classic k-Nearest Neighbors approach.  To estimate &rho;(t), we find the
k closest rows in our dataset to t, and average the Y values of those
data points.  Of course, k is a hyperparameter, chosen by the analyst.


</br>
</br>

# Bias and Variance of WHAT

## Bias in prediction

Though the reader may have seen the concept of statistical bias in terms
```{marginfigure}
Prediction methods are often used in real estate contexts, such as in
assessing market value.
```
of simple estimators, our context here is different.  Let's take as
our example prediction of house prices, say using the predictor
variables Square Feet and Location.

Generally, the larger a home is, the greater its value.  But the
same-sized home will have a higher value in some places than others.
In other words,

> If we predict the price of a house based only on size and decline to
> use (or do not know) its location, we induce a bias.  If say the house is
> in an expensive neighborhood, our prediction based only on Square Feet
> will likely be too low.

We can write this as

   average(actual value - predicted value from size alone, 
      given size=2125, location='Seaside') > 0 

That average discrepancy is called the *bias*.  Here the bias will be
positive for the houses in expensive neighborhoods, and negative for
those in inexpensive areas.

Note that "average" here means the mean over all possible houses of the
given size and place.  In some cases, we may not underpredict, but on
average we will do so.  The mean is also over possible training
datasets; in a linear model, for instance, different datasets will
produce different values of the b<sub>i</sub>, and thus produce
different predicted values.

## Variance in prediction

This refers to sampling variance, which measures the degree of
instability of one's estimated r(t) from one training dataset to another.
E.g. in the above linear model, there will the variation of the 
b<sub>i</sub> from one sample to another, thus causing variation in 
r(t) among samples.

The same holds for nonparametric models.  In the k-NN example above, say
n = 1000 and k = 25.  Then we are essentially basing our r(64.2) on the
average of 25 data points--whereas the linear model uses all 1000 data
points.  So, k-NN will have a larger variance.  

Note too that since k-NN averages k values, then for fixed n, we will
have that the larger k is, the smaller the variance.

The relevance of sampling variance is difficult for many nonspecialists
in statistics/machine learning to accept.  "But we only have one
sample," some might object to the analysis in the preceding paragraphs.
True, but we are interested in the probability that that our sample is
representative of the population.  In a gambling casino, you may play a
game just once, but you still would like to know the probability of
winning.  If the winnings vary rather little from one play of the game
to another, and if the expected value is positive, your risk is less.
But with large variability from one play to another, you will likely
feel uneasy about taking the chance.

The same is true in data science, and that in turn means that
sampling variance is key.

# A U-Shaped Curve

We stated above that a linear model cannot be exactly correct, even with
an unlimited amount of data.  So, why not a quadratic model?  

&rho;(weight) = mean weight = &beta;<sub>0</sub> + &beta;<sub>1</sub> height + &beta;<sub>2</sub> height<sup>2</sup>

This includes the linear model as a special case (&beta;<sub>2</sub> =
0), but also is more general.  In the population, the best-fitting
quadratic model will be more accurate than the best-fitting linear one.
How about one of degree 3?  With higher and higher degree polynomials,
we can reduce the bias to smaller and smaller amounts.

But in finite samples, the higher the degree of the polynomial, the
larger the variance.  (We have more parameters to estimate, and I like
to think in terms of them "sharing" the data, less data available to
each one.) 

So, we have a battle between bias and variance!  As we increase the
degree, first 1, then 2, then 3 and so on, the bias initially shrinks a
lot while the variance grows only a little.  But eventually, inceasing
the degree by 1 will reduce bias only a little, while variance increases
a lot.

Result:

> If we plot MSPE vs. degree, we typically will get a
> U-shaped curve.  Bias reduction dominates variance increase initially,
> but eventually variance overpowers bias.  Once we pass the low point
> of the curve, **we are overfitting**.

The same is true for k-NN, plotting prediction error against k.  As
noted, we can achieve smaller bias with smaller k.  The latter situation
means smaller neighborhoods, making the problem of "using tall people to
predict a short person's weight" less likely.  But smaller k means we
are taking an average over a small set of people, which will vary a lot
from one sample to another, i.e. larger variance.  If we try a range of
k values, the bias-variance tradeoff will typically result in a U-shaped
curve.

Again, MSPE is a population quantity, but we can estimate it via
cross-validation.  For instance, for each value of k in a range, we can
fit k-NN on our training data and the estimate the corresponding
MSPE value by predicting our test set.

Mean squared error is the sum of a decreasing quantity, squared bias,
and an increasing quantity, variance.  This typically produces a
U-shape, but there is no inherent reason that it must come out this way.
A double-U can occur (see below), or for that matter, multiple-U. 

</br>
</br>

# Empirical Illustration

I used the [Million Song
Dataset](https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd).  It
consists of 90 audio measurements on about 500,000 songs.  To reduce the
amount of computation, I used only a random subset of 10,000 songs, and
only two audio variables, fitting polynomial models in the two
predictors, with increasing polynomial degree.  (The package includes the dataset
**oneksongs**, a random subset of 1,000 songs.)

As noted, mean squared prediction error is a sum of bias squared and
variance.  I calculated mean absolute prediction error (MAPE), but the
principle is the same; it still combines bias and variance.  

My goal was to show that:

1.  As degree increases, MAPE at first falls but later rises, confirming the
    "U-shape" discussed above.

2.  But as degree increases, variance *always* increases.

Combining these two points, we can observe the battle between bias and
variance in their impact on MAPE.  At first the reduction in bias
dominates the increase in variance, but for larger degrees, the
opposite it true.

As a useful measure of variance, I took the first song in the dataset,
and let t = the vector of audio characterstics of that particular song.
I then found the variance of r(t) (a matrix quadratic form of t with the
estimated covariance matrix of the vector of coefficients of the
polynomial).

I fit degrees 1 through 12.  (Again, to save on computation, I used
interaction terms only of degree 2.)  For cross-validated MAPE values, I
used 20 random holdout sets of size 1000.  Here is the output, with
variance and MAPE in the top and bottom rows, respectively:

```text
           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]
[1,] 0.02440784 0.02718803 0.03184419 0.04229401 0.04945168 0.05450264
[2,] 7.86296820 7.74149419 7.74222432 7.66320622 7.70638321 7.69907202
           [,7]       [,8]     [,9]        [,10]    [,11]    [,12]
[1,] 0.06572677 0.07980077 0.114887 -0.008906266       NA       NA
[2,] 7.73367546 7.70928618 7.733230  7.793813681 9.980302 17.36951
```

Starting with degree 10, there are serious numerical issues, arising
from exact or nearly exact collinearity among the various terms in the
polynomial.  Thus variance is not reported for degrees 11 and 12, and
even is negative (due to roundoff error) for degree 10.

In the top row, one can see variance steadily increasing through 
degree 9.  MAPE shows a general "U" trend, albeit a
shallow one.  

# Overfitting with Impunity--and Even Gain?

Research in machine learning has, among other things, produced two major
mysteries, which we address now.

## Baffling behavior--drastic overfitting

In many ML applications, especially those in which neural networks are
used, there may be far more hyperparameters than data points, i.e. p >>
n.  Yet excellent prediction accuracy on new data has often been
reported in spite of such extreme overfitting.

## How could it be possible?

Much speculation has been made as to why these phenomena can occur.  My
own speculation is as follows.

* **These phenomena occur in classification problems in which there is a
very low error rate, 1 or 2% or even under 1%.**  

* In other words, the classes are widely separated, i.e. there is a
  *large margin* in the SVM sense.  Thus a large amount of variance 
  can be tolerated in the fitted model, and there is room for
  myriad high-complexity functions that separate the classes perfectly
  or nearly so, even for test data.

* Thus there are myriad fits that will achieve 0 training error, and the
  iterative solution method used by ML methods such as neural networks
  is often of a nature that the *minimum norm* solution is arrived at.
  In other words, of the ocean of possible 0-training error solutions,
  this one is smallest, which intuitively suggests that it is of small
  variance.  This may produce very good test error.

Here is an example, using a famous image recognition dataset
(generated by code available [here](https://jlmelville.github.io/uwot/metric-learning.html):

![alt text](UMAPFMNIST.png)

There are 10 classes, each shown in a different color.  Here the UMAP
method (think of it as a nonlinear PCA) was applied for dimension
reduction, down to dimension 2 here.  There are some isolated points here and
there, but almost all the data is separated into 10 "islands."  Between
any 2 islands, there are tons of high-dimensonal curves, say high-degree
polynomials, that one can fit to separate them.  So we see that
overfitting is just not an issue, even with high-degree polynomials.


## Baffling behavior--"double descent"

The phenomenon of *double descent* is sometimes observed, in which the
familiar graph of test set accuracy vs. model complexity consists of
*two* U shapes rather than one.  The most intriguing cases are those in
which the first U ends at a point where the model fully interpolates the
training data, i.e. 0 training error--and yet *even further* complexity
actually reduces test set error.

Below is an example again with the song data, but with a linear model
rather than a polynomial one.  The horizontal axis is p, the number of
predictors.  We see the double-U, though not with improvement in the
second U over the first.

![alt text](https://matloff.files.wordpress.com/2020/11/overfit.png)

## How could it be possible?

The argument explaining double descent generally made by researchers in
this field is actually similar to the argument I made for the
"overfitting with impunity" analysis above, regarding minimum-norm
estimators.  

By the way, the function **penroseLM()** in the regtools package (on
top of which qeML is built) does find the minimum-norm b in the case of
overfitting a linear model.  As mentioned, this has been observed
empirically, and some theoretical work has proved it under certain
circumstances.


