
---
title: "Feature Selection"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FeatureSelection}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Clearing the Confusion: Feature Selection 

In many applications we may have many features---dozens, hundreds or
even more.  In machine learning contexts, where we are interested primarily in
prediction, we typically don't want to use all the features, for a couple of
reasons:

* Avoiding overfitting.

    As we add more and more features to a model, bias is reduced but
    variance increases.  At some point, variance overwhelms bias, and our
    predictive ability declines.

* Avoiding large computation time and/or memory usage.

    Depending on the solution method used, computation time for a linear
    model increases with the cube, or at least the square, of the number
    of features.  Modern ML models can be even worse in terms of
    computational burden.  In addition, large models may result in lack
    of convergence or instability of the output.

# Which Method to Use?

Many, many methods for feature selection have been developed over the
years, and you will often hear someone claim that their favorite is *the*
one to use, The Best.  As you may guess, the fact that there are
conflicting claims as to "best" reflects the fact that no such Best exists.

We will cover a few feature selection methods here, to explain their
rationales and how to use them in qeML.

# How Many Is Too Many?

We will use standard notation here, with n and p denoting the number of
points in our dataset (i.e. number of rows) and the number of features,
respectively.  Important points to remember:

* The larger n is, then the larger a value of p that can be used before
  variance increase overwhelms bias reduction.

* However, that bias-variance tradeoff transition depends on the
  application. There is no magic formula for best p as a function of n.

* Nevertheless, a commonly-used rule of thumb is to limit p to at most
  n<sup>0.5</sup>.  Some theoretical analyses even suggest the more
  conservative rule p < log(n).  Again, these are oversimplifications,
  but some find them useful as guides to intuition.

* The value of p depends on "expanded" versions of any categorical features.

## Impact of categorical variables

Note that last point.carefully.  Consider the dataset **nycdata**
included in qeML, which is derived from taxi trip data made publicly
available by the New York City government.  Many public analyses have
been made on various versions of the NYC taxi data, frequently with the
goal of predicting trip time.  Let's take a look:

``` r
> data(nyctaxi)
> dim(nyctaxi)
[1] 10000     5
> head(nyctaxi)
        passenger_count trip_distance PULocationID DOLocationID PUweekday
2969561               1          1.37          236           43         1
7301968               2          0.71          238          238         4
3556729               1          2.80          100          263         3
7309631               2          2.62          161          249         4
3893911               1          1.20          236          163         5
4108506               5          2.40          161          164         5
        DOweekday tripTime
2969561         1      598
7301968         4      224
3556729         3      761
7309631         4      888
3893911         5      648
4108506         5      977
```

(Time-of-day, month etc. data is available but not in this particular dataset.)

So n = 10000, p = 5.  At first glance, one might guess that we could use
all 5 features.  Let's try a linear model:

``` r
> z <- qeLin(nyctaxi,'tripTime')
holdout set has  1000 rows
Warning messages:
1: In eval(tmp, parent.frame()) :
  7 rows removed from test set, due to new factor levels
2: In predict.lm(object, newx) :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newx) :
  prediction from a rank-deficient fit may be misleading
```

The term "rank-deficient" is referring to multicollinearity in the data,
i.e. strong relations among the features, producing a possibly unstable
result.

Let's look at the estimated beta coefficients:

``` r
> summary(z)

Call:
lm(formula = cbind(tripTime) ~ ., data = xy)

Residuals:
    Min      1Q  Median      3Q     Max
-3807.5  -186.0   -49.8   131.8  3179.2

Coefficients: (4 not defined because of singularities)
                 Estimate Std. Error t value Pr(>|t|)
(Intercept)     -2125.474    351.347  -6.049 1.51e-09 ***
trip_distance     163.056      1.670  97.628  < 2e-16 ***
PULocationID4    1424.775    345.770   4.121 3.81e-05 ***
PULocationID7    1212.331    349.196   3.472 0.000520 ***
PULocationID10    655.251    371.870   1.762 0.078097 .
PULocationID12   1829.276    408.229   4.481 7.52e-06 ***
PULocationID13   1271.201    338.220   3.758 0.000172 ***
PULocationID17   1167.109    477.898   2.442 0.014619 *
PULocationID24   1356.014    342.706   3.957 7.66e-05 ***
PULocationID25   1381.439    354.736   3.894 9.92e-05 ***
PULocationID26   1282.229    506.842   2.530 0.011429 *
PULocationID33   1353.187    363.058   3.727 0.000195 ***
PULocationID35    726.147    474.147   1.531 0.125687
PULocationID36    972.828    391.276   2.486 0.012927 *
...
PULocationID260   972.537    375.500   2.590 0.009614 **
PULocationID261  1334.116    340.437   3.919 8.97e-05 ***
PULocationID262  1362.842    337.713   4.036 5.50e-05 ***
PULocationID263  1343.784    337.113   3.986 6.77e-05 ***
PULocationID264  1378.455    341.150   4.041 5.38e-05 ***
PULocationID265  2163.796    370.248   5.844 5.27e-09 ***
DOLocationID3     869.664    339.996   2.558 0.010549 *
DOLocationID4     913.146    108.088   8.448  < 2e-16 ***
DOLocationID7    1065.241    105.990  10.050  < 2e-16 ***
DOLocationID10   1703.133    211.081   8.069 8.06e-16 ***
...
DOLocationID119  1077.036    274.406   3.925 8.74e-05 ***
DOLocationID121  1515.904    340.850   4.447 8.80e-06 ***
DOLocationID123        NA         NA      NA       NA
DOLocationID124  2707.981    339.513   7.976 1.70e-15 ***
DOLocationID125  1123.702    105.203  10.681  < 2e-16 ***
...
DOLocationID262   933.146     96.473   9.673  < 2e-16 ***
DOLocationID263   885.520     94.578   9.363  < 2e-16 ***
DOLocationID264   960.527    108.120   8.884  < 2e-16 ***
DOLocationID265   169.068    123.391   1.370 0.170666
DayOfWeek2         44.691     15.004   2.979 0.002903 **
DayOfWeek3        100.480     14.188   7.082 1.53e-12 ***
DayOfWeek4        105.223     13.990   7.522 5.95e-14 ***
DayOfWeek5        133.312     13.775   9.678  < 2e-16 ***
DayOfWeek6         97.030     14.461   6.710 2.07e-11 ***
DayOfWeek7         57.133     14.579   3.919 8.97e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 327.2 on 8663 degrees of freedom
Multiple R-squared:  0.7128,	Adjusted R-squared:  0.7017
F-statistic:    64 on 336 and 8663 DF,  p-value: < 2.2e-16

```

There were 266 pickup/dropoff locations.  That means 530
dummy-variable features.  Similarly, 6 more dummies for day of the week.
R's **lm** function, which qeLin wraps, automatically converts factors
to dummies.

So, p is not 5 after all; it's 5 + 529 + 6 = 540!

We are assuming no interactions here.  But if some pickup/dropoff
location combinations behave quite differently from others, we ought to
consider interaction terms.  These will have their own dummy variables,
hugely increasing p if we include them all.

Note too that the model was so unstable that the coefficient for
DOLocationID123 turned out to be NA.


# Remedies

## Desiderata

What should we look for in a "good" feature selection method?  We take
the following as goals:

* **Desired Property A:** The method should have predictive ability as a central goal.

* **Desired Property B:** The method should be reasonably easy to explain.

* **Desired Property C:** The method should produce an ordered sequence of
candidate models: Best single predictor; best pair of predictors; best
triplet etc. This means we will have at most p feature sets to check,
say running cross-validation on each one.  In *best subset selection*,
by contrast, one looks at all the 2<sup>p</sup> possible feature sets, a
prohibitively large number computationally.

## Methods based on p-values

A classic approach for linear and generalized linear models is to first
fit a full model, then retain only those features that are
"statistically significant".  In the **nyctaxi** taxi data shown above,
we would retain, for instance **PULocationID33** but not **PULocationID35**.

This would violate our Desired Property A above.  Use of p-values in
geneeral is problematic (see the "NoPVals" vignette), and this is no
exception.  Just because &beta;<sub>i</sub> is nonzero does not mean
that Feature i has strong predictive power; what if  &beta;<sub>i</sub>
is nonzero but tiny?

A common variation is to use a *stepwise* method.  In the *forward*
version, one starts with no features, but adds them one at a time.  At
each step, one tests for the &beta; coefficient being 0.  When no new
feature is found to be "significant," the process stops, and we use
whatever features managed to make it in to the model so far.  The
*backward* version starts with a full model, and removes features one at
a time.  Though this one may seem to be an improvement, in that it does
take into account that a feature may be useful individually but not if
similar features are already in the equation, it still suffers from
the fundamental problems of p-values.

## The LASSO

Here we fit a linear model, subject to the constraint that no estimated
&beta;<sub>i</sub> is larger than &lambda;, a hyperparameter.  Starting
at 0, we increase &lambda; to ever-larger values, having the effect that
one &beta;<sub>i</sub> becomes nonzero at a time.  This gives us a
linear sequence of candidate features, which is essentially a forward
stepwise method, but NOT based on p-values; instead, the decision of
which feature to use next is based on cross-validated mean squared
prediction error.  

In the end, one could use the final estimated &beta; vector
formed by the LASSO, or one could simply view the process as a means of
choosing a feature set for some other ML algorithm.

Here is the LASSO approach on the **nyctaxi** data:

``` r
> lassout <- qeLASSO(nyctaxi,'tripTime') 
> lassout$lambda.whichmin
[1] 51
> lassout$whenEntered
  trip_distance PULocationID.132 PULocationID.186      DayOfWeek.1
               2               29               31               31
DOLocationID.132 DOLocationID.124      DayOfWeek.5  DOLocationID.33
              33               34               34               35
DOLocationID.189 DOLocationID.225 PULocationID.213  PULocationID.76
              35               35               37               38
PULocationID.263   DOLocationID.1 DOLocationID.177      DayOfWeek.2
              38               38               38               38
PULocationID.124  DOLocationID.35  DOLocationID.36  DOLocationID.89
              39               39               39               39
DOLocationID.231 DOLocationID.263 PULocationID.239 DOLocationID.155
              39               39               40               40
 PULocationID.40  PULocationID.43 PULocationID.146 PULocationID.161
              41               41               41               41
PULocationID.230  DOLocationID.22  DOLocationID.37 DOLocationID.161
              41               41               41               41
DOLocationID.162 DOLocationID.251 PULocationID.163 PULocationID.211
              41               41               42               42
PULocationID.257  DOLocationID.49  DOLocationID.81 DOLocationID.179
              42               42               42               42
...
```

For each value of &lamda; run by the code, one or more features joined the
model.  The cross-valued mean squared prediction error reached its
minimum at the 51st &lamda;.  Along the way, **trip_distance** was the
first feature added, at the 2nd &lambda;, followed 
by PULocationID132 (29th), PULocationID186 (31st), 
DayOfWeek.1 (31st) and so o.


## Methods based on measures of feature importance


RF, Shapley

## Non-selection dimension reduction

Rather than doing feature selection *per se*, we might transform the
data to summary variables.  We could group the pickup/dropoff locations
by ZIP Code, for instance.  

Or, we could consider only the more frequently used locations.  The qeML
function **factorToTopLevels** would help here:

``` r
> newPUlocs <- factorToTopLevels(nyctaxi$PULocationID,m=0)
f
  3   8   9  14  15  16  18  19  20  22  23  28  29  32  34  47  53  55  56  59
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
 60  63  64  66  67  69  72  73  77  78  80  81  85  86  89  91  92  98 101 102
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
111 119 122 133 136 139 147 149 150 154 159 165 167 169 174 177 180 185 188 190 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
...

 75 211  50 144  13 148 158 151 137 114 233 262 143 113 231  43 246 249  90 229
 71  74  78  81  88  96  98 104 116 119 129 129 131 136 140 146 148 181 187 195
164 238 140  68 141 100 138 264 163 107 263 239  79 132 234 142 170  48 230 162
204 207 208 212 213 215 223 237 240 244 245 256 259 281 303 318 320 321 325 328
186 161 236 237
362 397 407 432
enter m: 40
> head(newPUlocs)
[1] 236 238 100 161 236 161
41 Levels: 100 107 113 114 13 132 137 138 140 141 142 143 148 151 158 ... other
```

Here, the option **m = 0** is interactive.  After looking at the
frequencies, we decided to take the most-frequent 40 locations.  Let's
take that number for the dropoffs too:

``` r
> newDOlocs <- factorToTopLevels(nyctaxi$DOLocationID,m=40)
```

Now use the new data:

``` r
> nyct$PULocationID <- newPUlocs
> nyct$DOLocationID <- newDOlocs
> head(nyct)
        trip_distance PULocationID DOLocationID tripTime DayOfWeek
2969561          1.37          236           43      598         1
7301968          0.71          238          238      224         4
3556729          2.80          100          263      761         3
7309631          2.62          161          249      888         4
3893911          1.20          236          163      648         5
4108506          2.40          161          164      977         5
> linout <- qeLin(nyct,'tripTime')
etc.

```

We might try further dimension reduction via
Principal Components Analysis (PCA), say

``` r
> pcaOut <- qePCA(nyct,'tripTime','qeLin',pcaProp=0.8)
```

This would apply PCA to the features, tranforming them to PCs.  It would
retain the most-variable PCs, up to the point at which they formed 80%
of the total variance, and then apply **qeLin** to the new variables.

