<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Unbalanced Class Data</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Unbalanced Class Data</h1>



<div id="clearing-the-confusion-unbalanced-class-data" class="section level1">
<h1>Clearing the Confusion: Unbalanced Class Data</h1>
<p>Many resources on machine learning (ML) classification problems recommend that if one’s dataset has unbalanced class sizes, one should modify the data to have equal class counts. Yet it is shown here that this is both unnecessary and often harmful. Alternatives are presented.</p>
<div id="outline" class="section level2">
<h2>Outline</h2>
<ul>
<li><p>Why is imbalance a problem? Our ML algorithm may never predict the small class, making the algorithm rather meaningless.</p></li>
<li><p>A popular solution is to artificially balance the data, e.g. CRAN package <strong>smotefamily</strong>.</p></li>
<li><p>This changes the class probabilities, with undesirable consequences. Since class probabilities do factor into our prediction, changing them distorts our analysis.</p></li>
<li><p>Related problem: the class probabilities may be wrong to begin with, either due to the original sampling scheme or because the probabilities may change.</p></li>
<li><p>Two problems will be addressed in this document:</p>
<ul>
<li><p>What can be done instead of artificially balancing the data?</p></li>
<li><p>What can we do to adjust data with the wrong class probabilities?</p></li>
</ul></li>
<li><p>Solutions:</p>
<ul>
<li><p>Regarding imbalance, we consider but reject ROC, and instead argue that informal use of conditional probabilities is best.</p></li>
<li><p>Regarding incorrect class probabilities, we develop an adjustment formula.</p></li>
</ul></li>
</ul>
</div>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>Illustrations of the (perceived) problems and offered remedies appear in numerous parts of the ML literature, ranging from <a href="https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data">Web tutorials</a> to <a href="https://link.springer.com/article/10.1186/s40537-018-0151-6#Sec2">the research literature</a>. Major packages, such as <a href="https://cran.r-project.org/package=caret">caret</a>, <a href="https://cran.r-project.org/package=parsnip">parsnip</a>, and <a href="https://cran.r-project.org/package=mlr3">mlr3</a>, also offer remedies.</p>
<p>All of these sources recommend that you artificially equalize the class counts in your data, via various resampling methods. Say for instance we are in the two-class case, and have class sizes of 20000 and 80000 for class 1 and class 0, respectively. Here are some ways to rebalance.</p>
<ul>
<li><p>Randomly discard 60000 data points from class 0.</p></li>
<li><p>Resample 80000 data points from the 20000, with replacement.</p></li>
<li><p>Using distribution approximation methods such as SMOTE, resample 80000 from the 20000.</p></li>
</ul>
<p>Upon closer inspection, though, one sees that <strong>this is generally inadvisable, indeed harmful,</strong> for several reasons:</p>
<ul>
<li><p>Undersampling is clearly problematic: Why throw away data? <strong>Discarding data weakens our ability to predict new cases.</strong></p></li>
<li><p>The data may be unbalanced <em>for a reason</em>. Thus the imbalance itself is useful information, again resulting in reduced predictive power if it is ignored.</p></li>
<li><p>There are <strong>principled alternatives to resampling,</strong> including an adjustment formula to be presented here.</p></li>
</ul>
<p>In other words:</p>
<blockquote>
<p>Resampling methods <strong>are both harmful and unnecessay</strong>.</p>
</blockquote>
</div>
<div id="motivating-examples" class="section level2">
<h2>Motivating Examples</h2>
<div id="credit-card-fraud-data" class="section level3">
<h3>Credit card fraud data</h3>
<p>This is a <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">Kaggle dataset</a>. Quoting from the Kaggle site,</p>
<blockquote>
<p>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.</p>
</blockquote>
<p>Note the phrase, “highly unbalanced.”</p>
<p>Due to privacy concerns, PCA has been used to replace most of the features, though two original features have been retained.</p>
</div>
<div id="missed-appointments-data" class="section level3">
<h3>Missed appointments data</h3>
<p>This is a <a href="https://www.kaggle.com/joniarroba/noshowappointments%5D">Kaggle dataset</a> on whether patients keep their medical appointments. The imbalance here is more mild, with about 20% of the patients being no-shows.</p>
</div>
<div id="optical-letter-recognition-data" class="section level3">
<h3>Optical letter recognition data</h3>
<p>This is a well-known <a href="https://archive.ics.uci.edu/ml/datasets/Letter+Recognition">UCI Machine Learning Repository dataset</a>. Again quoting from the site:</p>
<blockquote>
<p>The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.</p>
</blockquote>
<p>This dataset is close to balanced, with each letter appearing about 775 times.</p>
<p>The dataset is conveniently available in the <code>mlbench</code> package.</p>
</div>
<div id="mt.-sinai-hospital-x-ray-study" class="section level3">
<h3>Mt. Sinai Hospital X-ray study</h3>
<p>In an X-ray <a href="https://www.scientificamerican.com/article/rise-of-robot-radiologists">classification tudy</a> from Mount Sinai Hospital, the classification method worked well on the original hospital data, but not in prediction of new cases at other locations. The study’s authors found that an important factor underlying the discrepancy was that the class probabilities p<sub>i</sub> vary from one hospital to another. Here the class probabilities really do change, not artificially, but the issues are the same, and again an adjustment procedure would be desirable.</p>
</div>
<div id="cell-phone-fraud" class="section level3">
<h3>Cell phone fraud</h3>
<p>In <a href="https://link.springer.com/article/10.1023/A:1009700419189">a study by Fawcett and Provost</a>, the authors found that</p>
<blockquote>
<p>The level of fraud changes dramat-ically month-to-month because of modifications to work practices (both the carrier’s andthe bandits’).</p>
</blockquote>
</div>
</div>
<div id="terminology" class="section level2">
<h2>Terminology</h2>
<p>We refer to the class probabilities for given feature values as <em>conditional class probabilities</em>, because they are probabilities subject to conditions. If say we wish to classify a patient as to whether she has a certain disease or not, and we know her blood test value on some measure is 2.39, the latter is the condition. Among all patients with that test value, what proportion of them have the disease?</p>
<p>The overall class probabilities, e.g. the 0.000172 value above, are <em>unconditional class probabilities</em>.</p>
<p>The conditional and unconditional class probabilities are often referred to as the <em>posterior</em> and <em>prior</em> probabilities. This sounds Bayesian, and indeed we do use Bayes’ Rule, but there is no subjectivity involved. These are real probabilities. E.g. in a disease classification application, there is a certain proportion of people in the population who have the disease. This is a prior probability, which we will estimate from our training data.</p>
</div>
<div id="notation" class="section level2">
<h2>Notation</h2>
<ul>
<li>c = number of classes</li>
<li>Y = class label, 0,1,…,c-1</li>
<li>X = vector of features</li>
<li>p<sub>i</sub> = P(Y = i) (prior probs.)</li>
<li>q<sub>i</sub>(t) = P(Y = i | X = t) (posterior probs.)</li>
<li>Y<sub>pred</sub> = the value we predict for Y</li>
<li>(X<sub>i</sub>,Y<sub>i</sub>), i=1,2,…,n: training data</li>
<li>f<sub>i</sub>: (conditional) density of X within class i</li>
</ul>
<p>2-class case (Y = 0,1): * p: P(Y = 1) (probability of class 1) * r<sub>i</sub>: estimated q<sub>1</sub>(X<sub>i</sub>)</p>
<p>Note that for a given case, i.e. a given value of the feature vector X, the r<sub>i</sub> will sum to 1, as i ranges across all our classess.</p>
</div>
<div id="key-issue-how-were-the-data-generated" class="section level2">
<h2>Key issue: How were the data generated?</h2>
<p>The examples above illustrate two important cases:</p>
<ul>
<li><p>The fraud data is <em>imbalanced</em>, but <em>naturally so</em>. Assuming the two-day data collection period was typical, the population class probability for fraud will be about what we see in the data, 0.172%.</p></li>
<li><p>The LetterRecognition data is <em>balanced</em>, but only <em>artificially so</em>. The curator of the dataset wanted the data to have about the same number of instances of each letter. But in general English usage, letters occur with quite different frequencies:</p></li>
</ul>
<blockquote>
<p>E 12.02% <br> T 9.10% <br> A 8.12% <br> O 7.68% <br> I 7.31% <br> N 6.95% <br> … <br> … <br> Q 0.11 <br> J 0.10 <br> Z 0.07</p>
</blockquote>
<p>(<a href="http://www.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html">source</a>). One can obtain these numbers in <code>regtools</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">data</span>(ltrfreqs)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> lf <span class="ot">&lt;-</span> ltrfreqs</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> lf <span class="ot">&lt;-</span> ltrfreqs[,<span class="dv">2</span>] <span class="sc">/</span> <span class="dv">100</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">names</span>(lf) <span class="ot">&lt;-</span> ltrfreqs[,<span class="dv">1</span>]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># example</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> lf[<span class="st">&#39;A&#39;</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>     A </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0812</span> </span></code></pre></div>
</div>
<div id="optimal-classification-rules" class="section level2">
<h2>Optimal classification rules</h2>
<p>Consider the 2-class setting, with Y = 0,1. As above, denote the (conditional) probability that Y = 1 for a particular new case to classify by r<sub>i</sub>. We might have an estimate of r<sub>i</sub> from fitting a logistic model, for instance.</p>
<p>The rule that minimizes the overall probability of misclassification is:</p>
<p>Y<sub>pred</sub> = 1 if r<sub>1</sub> &gt;= 0.5</p>
<p>Y<sub>pred</sub> = 0 if r<sub>1</sub> &lt; 0.5</p>
<p>But note the criterion here, minimizing the overall probability of misclassification. Other criteria are possible. It may be, for instance, that we wish to place different weights on false positives and false negatives. In the credit card fraud case, a false negative may mean a major loss, much worse than what we lose with a false positive (wasted investigation time, etc.). We may define our loss in those terms, and seek to minimize expected loss.</p>
<p>Specifically, let l<sub>01</sub> be the loss we incur if we guess Y = 0 but Y = 1. In the opposite case – guess 1 but Y = 0 – say our loss is 1.0. (All that matters is the ratio of the two losses.) To minimize expected loss, the optimal rule can be shown to be (see Appendix A below)</p>
<p>Y<sub>pred</sub> = 1 if r<sub>i</sub> &gt; 1/(1+l<sub>10</sub>)</p>
<p>Y<sub>pred</sub> = 0 if r<sub>i</sub> &lt;= 1/(1+l<sub>10</sub>)</p>
<p>The loss value l<sub>01</sub> may be hard to quantify, and in any case, we argue below that any mechanical rule is overly constraining anyway.</p>
</div>
<div id="what-your-ml-algorithm-is-thinking" class="section level2">
<h2>What your ML algorithm is thinking</h2>
<p>ML algorithms take your data literally. Say you have a two-class setting, for Classes 0 and 1. If about 1/2 your data is Class 1, then the algorithm, whether directly or indirectly, operates under the assumption that the true population class probabilities are each about 0.5.</p>
<p>In the LetterRecognition data, since the sampling was actually <em>designed</em> to have about the same number of instances for each letter, the algorithm you use will then assume the true probabilities of the letters are about 1/26 each. We know that is false, as the table shown earlier illustrates.</p>
<p>So, if your sampling scheme artificially creates balanced data, as in the LetterRecognition data, or if you do resampling to make your data balanced, as is commonly recommended, you are fooling your ML algorithm.</p>
</div>
<div id="artificial-balance-wont-achieve-your-goals" class="section level2">
<h2>Artificial Balance Won’t Achieve Your Goals</h2>
<p>In fooling your algorithm, it will generate the wrong conditional class probabilities r<sub>i</sub> in our notation above. And whether we wish to minimize the overall probability of misclassification, or expected loss, or any other criterion, the algorithm will (again, directly or indirectly) rely on the values of r<sub>i</sub>.</p>
<p>Consider the fraud example, in which the data are highly imbalanced but in which wrongly guessing the large class carries heavy penalties for us. Recall our notation l<sub>01</sub> from above. As noted, this may be difficult to quantity, but for the moment, let’s suppose we can do so. What are the implications in terms of artificially balancing the data?</p>
<p>Actually, if we are going to be doing any adjustment of class sizes in our data, we should make the fraud class <em>larger</em> than the non-fraud class, not of equal size. How much larger will depend on the value of l<sub>01</sub>, but in any case, balancing the data will be wrong.</p>
<p>Frank Harrell <a href="https://www.fharrell.com/post/classification/">says it well</a>:</p>
<blockquote>
<p>For this reason the odd practice of subsampling the controls is used in an attempt to balance the frequencies and get some variation that will lead to sensible looking classifiers (users of regression models would never exclude good data to get an answer). Then they have to, in some ill-defined way, construct the classifier to make up for biasing the sample. It is simply the case that a classifier trained to a 1⁄2 [q = 1/2] prevalence situation will not be applicable to a population with a 1⁄1000 [p = 1/1000] prevalence.</p>
</blockquote>
</div>
<div id="so-what-should-be-done" class="section level2">
<h2>So, what SHOULD be done?</h2>
<p>Clearly, one’s course of action should center around the conditional class probabilities r<sub>i</sub>. (Note the plural; each case will have its own value of r<sub>i</sub>.) So, specifically, how should we use them? We will discuss two approaches:</p>
<ol style="list-style-type: decimal">
<li><p>Use of the ROC curve, which is derived from the r<sub>i</sub> values.</p></li>
<li><p>Informal, nonmechanical consideration of the r values.</p></li>
</ol>
<p>Our recommendation will be Approach 2 above.</p>
<div id="approach-1-use-the-roc-curve" class="section level3">
<h3>Approach 1: use the ROC curve</h3>
<p>Here one considers various threshhold values h, where we guess class 1 if r &gt; h, 0 otherwise. The value one chooses for h will determine the True Positive Rate and False Positive Rate:</p>
<p>TPR(h) = P(Y<sub>pred</sub> = 1 | Y = 1) = ∫<sub>t &gt; h</sub> f<sub>1</sub>(t) dt</p>
<p>FPR(h) = P(Y<sub>pred</sub> = 1 | Y = 0) = ∫<sub>t &gt; h</sub> f<sub>0</sub>(t) dt</p>
<p>The ROC curve is then a graph of TPR vs. FPR. As we vary h, it traces out the ROC curve. Some R packages, such as ROCR, colorize the curve, with the color showing the value of h. The qeML funcit</p>
<p>The idea here is that even if we cannot quantity l<sub>01</sub>, we can at least explore various values of h to produce a decision rule that roughly reflects the relative values we place on true and false positives.</p>
<p>Often a 45-degree line is superimposed on the ROC graph for comparison. To see why, say we are classifying patients as having a certain disease or not, say on the basis of a blood test. For a value of h associated with a point on the line, diseased and disease-free patients would have the same probability of testing positive, indicating that the test has no discriminatory value.</p>
<p><em>A note on AUC:</em> AUC is the total area under the ROC curve. As such, it is a measure of the general predictive ability of your algorithm on this data. This may seem attractive at first, but it is probably irrelevant in most applications, as it places equal weight on all possible TPR/FPR scenarios; usually we are far more interested in some settings than others. Note too that AUC values for original data vs. the artificially balanced data are not comparable.</p>
</div>
<div id="approach-2-informal-nonmechanical-consideration-of-the-ri-favored-choice" class="section level3">
<h3>Approach 2: informal, nonmechanical consideration of the r<sub>i</sub> (favored choice)</h3>
<p>The key points are:</p>
<ul>
<li><p>ROC involves macro-level quantities, TPR and FPR. In highly unbalanced settings, we are concerned with the micro-level, rare events where the much smaller class occurs.</p></li>
<li><p>Mechanical rules are too constraining for many applications.</p></li>
</ul>
<p>This latter point is especially relevant. Often the required decision is too critical to be left up to a machine. For instance, relevant to our fraud example, in <a href="https://www.pwc.com.au/consulting/assets/risk-controls/fraud-control-jul08.pdf">Fraud: a Guide to Its Prevention, Detection and Investigation</a> by Price Waterhouse Coopers, it is pointed out that</p>
<blockquote>
<p>… every fraud incident is different, and reactive responses will vary depending on the facts that are unique to each case.</p>
</blockquote>
<p>Use of a machine learning algorithm to make a mechanical decision would obviate this needed flexibility.</p>
<p>Instead, a practical, <em>effective</em> approach would be to simply look at the r values for the new cases directly. In other words, we have the algorithm only take the first step, after which we work “by hand” rather than by computer. We would still set a threshold h, yes, but would use it only as first-stage screening, with the second stage being done by humans.</p>
<p>At that point, the (human) auditor would take into account both that estimated probability – now worrying not only that it is larger than h but also <em>how much larger</em> – as well as such factors as the amount of the charge, special characteristics not measured in the available features, and so on. The auditor may not give priority, for instance, to a case for which the probability is above h but the monetary value of the transaction is small. On the other hand, if the probability is far above h, the auditor may give this transaction a closer look even if the monetary value is small.</p>
<p>A related issue is that of ranking. In the credit card fraud example, for instance, we may have sufficient audit funding to investigate 100 transactions per month. Thus we would rank the r<sub>i</sub>, and cull out the top 100 cases.</p>
</div>
</div>
<div id="obtaining-the-ri" class="section level2">
<h2>Obtaining the r<sub>i</sub></h2>
<p>Some ML algorithms naturally provide the r<sub>i</sub>, while others do not.</p>
<p>Below is an example for the logistic model, for a data frame <strong>ccf</strong> containing the credit card fraud data, with class variable <strong>Class</strong>. Say we take h = 0.25. We’ll cull out the cases with at least that probability of fraud, for preparation of investigation “by hand.”</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> glmout <span class="ot">&lt;-</span> <span class="fu">glm</span>(Class <span class="sc">~</span> .,<span class="at">data=</span>ccf,<span class="at">family=</span>binomial)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> condProbs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glmout,ccf,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> toCheck <span class="ot">&lt;-</span> <span class="fu">which</span>(condProbs <span class="sc">&gt;</span> <span class="fl">0.25</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">names</span>(toCheck) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">head</span>(toCheck)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>]  <span class="dv">542</span> <span class="dv">6109</span> <span class="dv">6332</span> <span class="dv">6335</span> <span class="dv">6337</span> <span class="dv">6339</span></span></code></pre></div>
<p>So we’d check cases 542, 6109 and so on by hand.</p>
<p>On the other hand, the SVM method does not produce the r<sub>i</sub>. In addition, even the r<sub>i</sub> produced by, e.g. <strong>glm()</strong> may have biases on the edges of the data. Thus an external method is needed.</p>
<p>Many implementations of SVM use a method known as <em>Platt scaling</em>. This assumes a logistic model from the regression function of Y (2-class case) against the SVM scores. In <code>regtools</code>, we instead use an method we developed, which performs a k-Nearest Neighbors regression of Y against the scores. (In non-SVM cases, the scores can be anything used by the ML method at hand to predict Y.) Our method is implemented in the <code>regtools</code> function <code>scoresToProbs()</code>.</p>
<div id="example-missed-apppointments-data" class="section level3">
<h3>Example: Missed Apppointments Data</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> ma <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&#39;KaggleV2-May-2016.csv&#39;</span>,<span class="at">header=</span>T,<span class="at">stringsAsFactors=</span>T)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> ma <span class="ot">&lt;-</span> ma[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">7</span>)]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> svmout <span class="ot">&lt;-</span> <span class="fu">qeSVM</span>(ma,<span class="st">&#39;No.show&#39;</span>,<span class="at">holdout=</span><span class="cn">NULL</span>)</span></code></pre></div>
<p>Let’s predict a hypothetical patient with the characteristics of patient 8 in our training data, but who is male:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> newx <span class="ot">&lt;-</span> ma[<span class="dv">8</span>,<span class="sc">-</span><span class="dv">9</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> newx<span class="sc">$</span>Gender <span class="ot">&lt;-</span> <span class="st">&#39;M&#39;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">predict</span>(svmout,newx)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>predClasses</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a> <span class="dv">8</span> </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>No </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>Levels<span class="sc">:</span> No Yes</span></code></pre></div>
<p>We predict him to show up for the appointment. But what is the probability of that?</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">predict</span>(svmout,newx,<span class="at">k=</span><span class="dv">75</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>predClasses</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a> <span class="dv">8</span> </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>No </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>Levels<span class="sc">:</span> No Yes</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span>probs</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            No       Yes</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,] <span class="fl">0.8133333</span> <span class="fl">0.1866667</span></span></code></pre></div>
<p>What happened here? Our function <code>predict.qeSVM</code> includes this code:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(k)) {</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>        y <span class="ot">&lt;-</span> object<span class="sc">$</span>y</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        trnScores <span class="ot">&lt;-</span> object<span class="sc">$</span>decision.values</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        newScores <span class="ot">&lt;-</span> <span class="fu">getDValsE1071</span>(object, newx)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        probs <span class="ot">&lt;-</span> <span class="fu">scoresToProbs</span>(y, trnScores, newScores, k)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        res<span class="sc">$</span>probs <span class="ot">&lt;-</span> probs</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    }</span></code></pre></div>
<p>Armed with this probability estimate for the given patient, we can make an informal decision as to whether to invest time and effort to make sure he does show up, say by phone call reminders, additional text messages etc.</p>
<p>And what about ranking? We would call <strong>predict()</strong> on our current set of new cases, then rank the resulting probabilities. Just as an example, let’s take our training set as the “new cases.”</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(svmout,svmout<span class="sc">$</span>x,<span class="at">k=</span><span class="dv">75</span>) </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> probs <span class="ot">&lt;-</span> preds<span class="sc">$</span>probs</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> probsOrder <span class="ot">&lt;-</span> <span class="fu">order</span>(probs) </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> probsSorted <span class="ot">&lt;-</span> probs[probsOrder]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">head</span>(probsSorted,<span class="dv">100</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>] <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">7</span>] <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a> [<span class="dv">13</span>] <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.01333333</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a> [<span class="dv">19</span>] <span class="fl">0.01333333</span> <span class="fl">0.01333333</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a> [<span class="dv">25</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a> [<span class="dv">31</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a> [<span class="dv">37</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a> [<span class="dv">43</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a> [<span class="dv">49</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a> [<span class="dv">55</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a> [<span class="dv">61</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a> [<span class="dv">67</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a> [<span class="dv">73</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a> [<span class="dv">79</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a> [<span class="dv">85</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a> [<span class="dv">91</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a> [<span class="dv">97</span>] <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span> <span class="fl">0.02666667</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">head</span>(probsOrder,<span class="dv">100</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>] <span class="dv">111857</span> <span class="dv">116081</span> <span class="dv">116105</span> <span class="dv">117394</span> <span class="dv">121485</span> <span class="dv">132575</span> <span class="dv">139922</span> <span class="dv">141078</span> <span class="dv">156201</span> <span class="dv">158322</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a> [<span class="dv">11</span>] <span class="dv">166633</span> <span class="dv">169521</span> <span class="dv">172965</span> <span class="dv">174563</span> <span class="dv">191263</span> <span class="dv">191534</span> <span class="dv">193848</span> <span class="dv">194199</span> <span class="dv">200048</span> <span class="dv">211445</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a> [<span class="dv">21</span>] <span class="dv">111474</span> <span class="dv">111529</span> <span class="dv">113535</span> <span class="dv">113572</span> <span class="dv">113577</span> <span class="dv">113579</span> <span class="dv">113582</span> <span class="dv">113584</span> <span class="dv">113585</span> <span class="dv">113590</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a> [<span class="dv">31</span>] <span class="dv">113593</span> <span class="dv">113598</span> <span class="dv">113617</span> <span class="dv">113634</span> <span class="dv">113636</span> <span class="dv">113637</span> <span class="dv">113640</span> <span class="dv">114420</span> <span class="dv">115955</span> <span class="dv">116040</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a> [<span class="dv">41</span>] <span class="dv">117506</span> <span class="dv">117538</span> <span class="dv">117540</span> <span class="dv">117542</span> <span class="dv">117543</span> <span class="dv">117546</span> <span class="dv">117548</span> <span class="dv">117551</span> <span class="dv">117555</span> <span class="dv">117562</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a> [<span class="dv">51</span>] <span class="dv">117565</span> <span class="dv">117628</span> <span class="dv">117634</span> <span class="dv">117638</span> <span class="dv">117639</span> <span class="dv">117640</span> <span class="dv">117641</span> <span class="dv">117648</span> <span class="dv">117651</span> <span class="dv">117663</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a> [<span class="dv">61</span>] <span class="dv">117693</span> <span class="dv">117715</span> <span class="dv">117718</span> <span class="dv">117773</span> <span class="dv">117842</span> <span class="dv">117843</span> <span class="dv">117845</span> <span class="dv">117847</span> <span class="dv">117849</span> <span class="dv">117853</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a> [<span class="dv">71</span>] <span class="dv">117856</span> <span class="dv">117956</span> <span class="dv">117962</span> <span class="dv">117966</span> <span class="dv">117969</span> <span class="dv">117970</span> <span class="dv">117971</span> <span class="dv">117979</span> <span class="dv">117982</span> <span class="dv">118010</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a> [<span class="dv">81</span>] <span class="dv">118013</span> <span class="dv">118015</span> <span class="dv">118018</span> <span class="dv">118021</span> <span class="dv">118024</span> <span class="dv">118026</span> <span class="dv">118027</span> <span class="dv">118037</span> <span class="dv">118038</span> <span class="dv">118041</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a> [<span class="dv">91</span>] <span class="dv">118043</span> <span class="dv">118048</span> <span class="dv">118112</span> <span class="dv">118125</span> <span class="dv">118127</span> <span class="dv">118130</span> <span class="dv">118131</span> <span class="dv">118133</span> <span class="dv">118138</span> <span class="dv">118139</span></span></code></pre></div>
<p>So we might investigate case numbers 111857, 116081 and so on.</p>
</div>
</div>
<div id="adjusting-the-pi" class="section level2">
<h2>Adjusting the p<sub>i</sub></h2>
<p>As noted in the LetterRecognition data example, in some cases the data are artificially balanced to begin with, due to the sampling design. Or we may have a situation like that of the Mt. Sinai Hospital radiology data, in which the class probabilities may change from one site to another, or may change over time as in the cell phone fraud example. Thus, our estimated values of the p<sub>i</sub> will be wrong. Can we fix that?</p>
<div id="the-adjustment-formula" class="section level3">
<h3>The adjustment formula</h3>
<p>For simplicity, we’ll assume the two-class setting here, with Class 0 and Class 1. This is the code for adjustment (the function is part of the <code>regtools</code> package):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>classadjust <span class="ot">&lt;-</span> <span class="cf">function</span> (econdprobs, wrongprob1, trueprob1) </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    wrongratio <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">-</span> wrongprob1)<span class="sc">/</span>wrongprob1</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    fratios <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>econdprobs <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>wrongratio)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    trueratios <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">-</span> trueprob1)<span class="sc">/</span>trueprob1</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> trueratios <span class="sc">*</span> fratios)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="er">}</span></span></code></pre></div>
<p>where</p>
<ul>
<li><p><code>condprobs</code> is the vector of conditional class probabilities for the dataset at hand, reported by the software applied to that data using incorrect p<sub>i</sub></p></li>
<li><p><code>wrongratio</code> is the ratio of the numbers of Class 0 to Class 1 datapoints in our dataset</p></li>
<li><p><code>trueratio</code> is the actual such ratio</p></li>
</ul>
<p>The return value is the set of adjusted conditional class probabilities.</p>
<p>For instance, suppose we are in a setting in which there are equal numbers of the two classes in our dataset, yet we know the true (unconditional) class probabilities are 0.2 and 0.8 for Classes 0 and 1. Then <code>wrongratio</code> would be 0.5/0.5 = 1.0, and <code>trueratio</code> would be 0.2/0.8 = 0.25.</p>
</div>
</div>
</div>
<div id="the-case-of-balanced-data-but-unknown-true-class-probabilities" class="section level1">
<h1>The case of balanced data but unknown true class probabilities</h1>
<p>What if the data are balanced but the true unconditional class probabilities are unknown? Other than creating various scenarios involving the true values and exploring the results, there is not much that we can do. In essence, the result is a maximum-likelihood kind of situation: Our predicted class will be the one whose (nominal) conditional probability makes our feature data most likely given the class (see Appendix B below), <strong>which is very different from the question we want to ask, Which class is most likely given the feature set?</strong> We’ll get answers (“Hey, the computer said such and such!”), but those answers will be of questionable value unless the predictors have very strong predictive power.</p>
<p>Note that setting utility values for “false positive,” “false negative” and so on does not solve the problem. One still needs to factor in the class probabilities in order to maximize expected utility.</p>
<p>Once again, <strong>balancing the data will not help.</strong></p>
<div id="summary" class="section level2">
<h2>Summary</h2>
<ul>
<li><p>There’s really no need to artificially balance your data, and doing so could be harmful.</p></li>
<li><p>Please don’t throw away data.</p></li>
<li><p>Conversely, creating extra data by up-sampling distorts the analysis and may undermine our predictive power.</p></li>
<li><p>If your data is “naturally” unbalanced, meaning it reflects population structure as in the credit card fraud example, don’t artificially force balance. Instead, choose a threshhold for conditional probabilities, and flag new cases that exceed it.</p></li>
<li><p>If your data is unrealistically balanced, as in the LettersRecognition example, and the true unconditional class probabilities are known, use the adjustment formula to convert the reported unconditional probabilities to realistic ones, and classify using them.</p></li>
<li><p>If your data is unrealistically balanced but the true unconditional class probabilities are unknown, recognize that your ML analysis may have only a very restricted interpretation and value.</p></li>
</ul>
</div>
<div id="appendix-a-derivation-of-the-unequal-loss-rule" class="section level2">
<h2>Appendix A: derivation of the unequal-loss rule</h2>
<p>Say the random variable W takes on the values 0,1, with P(W = 1) = r. (In the above, W plays the role of Y, conditioned on X.) Let’s compute the expected loss under two strategies:</p>
<ul>
<li>We guess W = 0.</li>
</ul>
<p>Then our loss is 0 P(W = 0) + l<sub>01</sub> P(W = 1) = l<sub>01</sub> r.</p>
<ul>
<li>We guess W = 1.</li>
</ul>
<p>Then our loss is 1 P(W = 0) + 0 P(W = 1) = 1-r.</p>
<p>So our optimal stragegy is to guess W = 1 if and only if 1-r &lt; l<sub>01</sub> r. In other words,</p>
<blockquote>
<p>Guess W = 1 if and only if r &gt; 1/(1+l<sub>01</sub>).</p>
</blockquote>
</div>
<div id="appendix-b-derivation-of-the-adjustment-formula" class="section level2">
<h2>Appendix B: derivation of the adjustment formula</h2>
<p>(For ease of notation etc., no distinction will be made here between sample and population quantities.)</p>
<p>Say there are two classes, labeled 1 and 0. Let Y denote the label and X denote the features, and say we have a new case with X = t. Then the true equation is</p>
<p>P(Y = 1 | X = t) = p f<sub>1</sub>(t) / [p f<sub>1</sub>t) + (1-p) f<sub>0</sub>(t)]<br />
<br> (Eqn. 1)</p>
<p>where p is P(Y = 1), the true unconditional probability of Class 1, and f<sub>i</sub>(t) is the conditional density of X within Class i. (If X is a discete random variable, substitute a probability for f.)</p>
<p>Rewrite the above as</p>
<p>P(Y = 1 | X = t) = 1 / [1 + {(1-p)/p} f<sub>0</sub>(t) / f<sub>1</sub>(t)] <br> (Eqn. 2)</p>
<p>Now suppose the analyst artificially changes the class counts in the data (or, as in the LetterRecognition example, the data is artificially sampled by design), with proportions q and 1-q for the two classes. In the case of artificially equalizing the class proportions, we have q = 0.5. Then the above becomes, in the eyes of your ML algorithm,</p>
<p>P(Y = 1 | X = t) = 1 / [1 + {(1-q)/q} f<sub>0</sub>(t) / f<sub>1</sub>(t)] <br> (Eqn. 3)</p>
<p>As noted earlier, what the ML algorithm is computing, directly or indirectly, is P(Y = 1 | X = t). Moreover, as also noted earlier, even <code>caret</code> and <code>mlr3</code> do make these quantities available for the various t, so we can solve for f<sub>0</sub>(t) / f<sub>1</sub>(t):</p>
<p>f<sub>0</sub>(t) / f<sub>1</sub>(t) = [g(t)-1] q/(1-q) <br> (Eqn. 4)</p>
<p>where</p>
<p>g(t) = 1 / P(Y = 1 | X = t)</p>
<p>Keep in mind that the g values are the ones output by our ML algorithm. Setting t to the matrix of our feature values, we can plug t into Eqn. 4 to get the vector of f<sub>0/f<sub>1</sub></sub> values, and plug the result into Eqn. 2 to obtain the vector of correct conditional probabilities.</p>
<p>We can now substitute in (Eqn. 2) from (Eqn. 4) to get the proper conditional probability.</p>
<p>The general m-class case. classes 0,1,…,m-1, actually reduces to the 2-class case, because</p>
<p>P(Y = i | X = t)</p>
<p>can be viewed as the conditional probability of class i vs. all other classes.</p>
<p>For each j = 1,…,c, denote by q<sub>j</sub> and p<sub>j</sub> the probabilities assumed by our ML algorithm, and the actual probabilities, respecitvely. For example, if the data are artificially balanced, either by resampling or by the original sampling design, we have q<sub>j</sub> = 1/c for all j.</p>
<p>To guide our intuition, note first that Eqn. 2 is now</p>
<p>P(Y = j | X = X<sub>i</sub>) = 1 / [1 + {(1-p<sub>j</sub>)/p<sub>j</sub>} [Σ<sub>m ≠ j</sub> f<sub>m</sub>(X<sub>i</sub>)] / f<sub>j</sub>(X<sub>i</sub>) <br> Eqn. 5</p>
<p>Now to handle the multiclass case, for each j, j = 1,…,c and each i = 1,…,n, do:</p>
<ol style="list-style-type: decimal">
<li>Set g<sub>ij</sub> to the reciprocal of the outputted estimated value of P(Y = j | X = X<sub>i</sub>),</li>
</ol>
<p>g<sub>ij</sub> = 1 / P(Y = j | X = X<sub>i</sub>),</p>
<ol start="2" style="list-style-type: decimal">
<li>Set</li>
</ol>
<p>[Σ<sub>m ≠ j</sub> f<sub>m</sub>(X<sub>i</sub>)] / f<sub>j</sub>(X<sub>i</sub>) = [g<sub>ij</sub> - 1] q<sub>j</sub> / (1 - q<sub>j</sub>) <br> Eqn. 6</p>
<ol start="3" style="list-style-type: decimal">
<li>Plug the result of Eqn. 6, along with the true p<sub>j</sub> ,into Eqn. 5 to obtain the true estimated conditional class probabilities.</li>
</ol>
</div>
<div id="appendix-c-what-is-really-happening-if-you-use-equal-class-probabilities" class="section level2">
<h2>Appendix C: What is really happening if you use equal class probabilities?</h2>
<p>Say we have balanced data, so the q<sub>i</sub> = 1/m for each i. Then in predicting the class of a new case having X = t, Equation 1 becomes</p>
<p>P(Y = i | X = t) = f<sub>i</sub>(t) / [ Σ<sub>j</sub> f<sub>j</sub>t) ]</p>
<p>for i = 0,1,…,m-1, since all the 1/m factors cancel.</p>
<p>This shows that our guessed class is</p>
<p>j = arg max<sub>i</sub> f<sub>i</sub>(t)</p>
<p>In other words, you are in effect asking, “Within which class j would our data X = t be most likely?” That is, we are maximizing (say in the discrete case)</p>
<p>P(X = t | Y = j)</p>
<p>over j. That’s completely different from the question we really are interested in, “Which class j is most likely, given X = t?”, i.e. maximizing</p>
<p>P(Y = j | X = t )</p>
<p>But it does show that if we artificially equalize the class sizes, we are finding the Maximum Likelihood Estimate of j, <strong>if the p<sub>i</sub> are unknown.</strong></p>
<p>If we really don’t know the true class probabilities p<sub>i</sub>, and artificially equalize the class sizes, we are at least getting a kind of MLE. However, what then is the practical meaning? Unless you are a subjective Bayesian (I am not), setting a flat prior, there is not much here.</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
