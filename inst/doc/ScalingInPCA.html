<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Should One Do SScaling in PCA?</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>






<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Should One Do SScaling in PCA?</h1>



<div id="clearing-the-confusion-scaling-in-pca" class="section level1">
<h1>Clearing the Confusion: Scaling in PCA</h1>
<p>Many resources on machine learning (ML) methodology recommend, or even state as crucial, that one <em>scale</em> (or <em>standardize</em>) one’s data, i.e. divide each variable by its standard deviation (after subtracting the mean), before applying Principal Component Analysis (PCA). Here we will show why that can be problematic, and provide alternatives.</p>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>The recommendation to scale is common. Here are some examples:</p>
<ul>
<li><p>R <strong>prcomp()</strong> man page</p>
<p>They say “scaling is advisable”:</p></li>
</ul>
<blockquote>
<p>scale.: a logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place. The default is ‘FALSE’ for consistency with S, but in general scaling is advisable.</p>
</blockquote>
<ul>
<li><p><a href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html">Scikit-Learn</a>:</p>
<p>Actually, the mention of normal distributions is misleading and in any case not relevant, but again there is a rather imperative statement to scale:</p></li>
</ul>
<blockquote>
<p>Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.</p>
</blockquote>
<ul>
<li><a href="https://www.datacamp.com/community/tutorials/pca-analysis-r">DataCamp</a></li>
</ul>
<p>Again, their phrasing is rather imperative:</p>
<blockquote>
<p>Note that the units used [in the <strong>mtcars</strong> dataset] vary and occupy different scales…You will also set two arguments, center and scale, to be TRUE.</p>
</blockquote>
<ul>
<li><p><a href="https://cran.r-project.org/package=caret">caret</a>, <strong>preProcess</strong> man page</p>
<p>Scaling done unless you say no:</p></li>
</ul>
<blockquote>
<p>If PCA is requested but centering and scaling are not, the values will still be centered and scaled.</p>
</blockquote>
<ul>
<li><a href="https://www.gastonsanchez.com/visually-enforced/how-to/2012/06/17/PCA-in-R/">Visually Enforced</a></li>
</ul>
<p>The word “must” is used here:</p>
<blockquote>
<p>Since most of the times the variables are measured in different scales, the PCA must be performed with standardized data (mean = 0, variance = 1).</p>
</blockquote>
</div>
<div id="the-perceived-problem" class="section level2">
<h2>The perceived problem</h2>
<p>As the DataCamp statement notes, some data may be “large” while other data are “small.” There is a concern that, without scaling, the large ones will artificially dominate. This is especially an issue in light of the variation in measurement systems – should a variable measured in kilometers be given more weight than one measured in miles?</p>
</div>
<div id="motivating-counterexample" class="section level2">
<h2>Motivating counterexample</h2>
<p>Consider a setting with two independent variables, A and B, with means 100, and with Var(A) = 500 and Var(B) = 2. Let A’ and B’ denote these variables after centering and scaling.</p>
<p>PCA is all about removing variables with small variance, as they are essentially constant. If we work with A and B, we would of course use only A. <strong>But if we work with A’ and B’, we would use both of them, as they both have variance 1.0.</strong> Scaling has seriously misled us here.</p>
</div>
<div id="alternatives" class="section level2">
<h2>Alternatives</h2>
<p>The real goal should be to make the variables <em>commensurate</em>. Standardizing to mean 0, variance 1 is not the only way one can do this. Consider the following alternatives.</p>
<ul>
<li><p>Do nothing. In many data sets, the variables of interest are already commensurate. Consider survey data, say, with each survey question asking for a response on a scale of 1 to 5. No need to transform the data here, and worse, standardizing would have the distortionary effect of exaggerating rare values in items with small variance.</p></li>
<li><p>Map each variable to the interval [0,1], i.e. t -&gt; (t-m)/(M-m), where m and M are the minimum and maximum values of the given variable. This is typically better than standardizing, but it does have some problems. First, it is sensitive to outliers. This might be ameliorated with a modified form of the transformation (and ordinary PCA has the same problem), but a second problem is that new data – new data in prediction applications, say – may stray from this [0,1] world.</p></li>
<li><p>Instead of changing the <em>standard deviation</em> of a variable to 1.0, change its <em>mean</em> to 1.0. This addresses the miles-vs.-kilometers concern more directly, without inducing the distortions I described above. And if one is worried about outliers, then divide the variable by the median or other trimmed mean.</p></li>
</ul>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
